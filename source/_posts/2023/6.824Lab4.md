---
title: MIT 6.824 Lab4
date: 2023-05-05 23:59:59
categories: Lab
tags: 
- distributed system
---

Lab4的任务是在Lab2的基础上实现一个可迁移可配置的多节点KV存储系统。

<!-- more -->

Lab4的实现难度很高，尤其是动态迁移数据这部分，实现与调试阶段都很痛苦，很难找出是哪一步导致没法实现线性一致性。整个设计对分布式系统的认识与设计能力都有很高的要求。这里参考了很多这篇文章的思路：[lab4.md](https://github.com/OneSizeFitsQuorum/MIT6.824-2021/blob/master/docs/lab4.md#%E5%88%86%E7%89%87%E7%BB%93%E6%9E%84)。也参考了一下这篇文章中的一些思路：[MIT6.824-lab4-shardKV](https://sworduo.github.io/2019/08/16/MIT6-824-lab4-shardKV/)。

# Lab4A
Lab4A的任务目标是在Lab2的基础上实现一个高可用配置中心shardmaster。最核心的部分是通过raft实现这四个rpc方法：
- Join：向不同的组中添加server，然后产生一个新的config，使shard尽可能平均分配给各个组，且移动的shard最少
- Leave：移除某一组，重分配它们的shard。同样需要使shard尽可能平均
- Move：指定将某一个shard移动到某一个组 
- Query：返回最新的配置

这四个方法对应的配置数据结构是项目提供的：
```go
type Config struct {
	Num    int              // config number
	Shards [NShards]int     // shard -> gid
	Groups map[int][]string // gid -> servers[]
}
```
Num表示配置的递增id，Shards表示的是某个shard被分配给哪个group，Groups表示的是某一个群组中有哪些server。这个lab里shard总数是固定的值，`NShards=10`。

实现结果为：
```text
# GO111MODULE=off go test

Test: Basic leave/join ...
  ... Passed
Test: Historical queries ...
  ... Passed  
Test: Move ...
  ... Passed
Test: Concurrent leave/join ...
  ... Passed
Test: Minimal transfers after joins ...
  ... Passed
Test: Minimal transfers after leaves ...
  ... Passed
Test: Multi-group join/leave ...
  ... Passed
Test: Concurrent multi leave/join ...
  ... Passed
Test: Minimal transfers after multijoins ...
  ... Passed
Test: Minimal transfers after multileaves ...
  ... Passed
PASS
ok      .../MIT6.824/src/shardmaster  8.896s
```

首先实现第一步是把lab2的框架都拷过来，只是将get和put、append方法改为新的rpc方法。

第二步是设计shardmaster和op结构：
```go
type ShardMaster struct {
	mu      sync.Mutex
	me      int
	rf      *raft.Raft
	applyCh chan raft.ApplyMsg

	// Your data here.
	prevOperation map[int64]CommandResponse
	notifyChanMap map[int]chan CommandResponse

	configs []Config // indexed by config num
}

type Op struct {
	// Your data here.
	Servers   map[int][]string // for Join
	GIDs      []int            // for Leave
	Shard     int              // for Move
	GID       int              // for Move
	Num       int              // for Query
	Type      string
	ClientId  int64
	CommandId int
}

type CommandResponse struct {
	CommandId int
	Config    Config
}
```

为什么op结构是这样的，需要结合这4个方法的参数来解释。实际上lab4A的核心就是实现这四个方法。下面逐一介绍

### Join
```go
type JoinArgs struct {
	Servers map[int][]string // new GID -> servers mappings

	ClientId  int64
	CommandId int
}
```

Join方法的参数是一个map。Join方法首先要把参数里的server加到group中去
```go
//...
	newConfig := Config{len(sm.configs), lastConfig.Shards, deepCopy(lastConfig.Groups)}
	for gid, servers := range groups {  // 这里的groups就是参数中的那个map
		if _, ok := newConfig.Groups[gid]; !ok {
			newServers := make([]string, len(servers))
			copy(newServers, servers)
			newConfig.Groups[gid] = newServers
		}
	}
//...
```

然后进行shard的重分配。这部分leave方法也会用到。
```go
//...
    s2g := Group2Shards(newConfig)
	log(fmt.Sprintf("%+v", s2g), 1)
	for {
        // 这个部分会找出哪个group的shard数量最多，哪个group的shard数量最少
        // 然后最多的给一个shard给最少的，循环直到相差数小于等于1
		source, target := GetGIDWithMaximumShards(s2g), GetGIDWithMinimumShards(s2g) 
		log(fmt.Sprintf("source %+v s2g[source] %v target %+v ", source, s2g[source], target), 1)
		if source != 0 && len(s2g[source])-len(s2g[target]) <= 1 {
			break
		}
		s2g[target] = append(s2g[target], s2g[source][0])
		s2g[source] = s2g[source][1:]
	}
	log(fmt.Sprintf("%+v", s2g), 1)
	var newShards [NShards]int
	for gid, shards := range s2g {
		for _, shard := range shards {
			newShards[shard] = gid
		}
	}
//...

// 这个方法的返回值是一个map，key为gid，value为每个group下的shard
func Group2Shards(config Config) map[int][]int {
	s2g := make(map[int][]int)

	for i, gid := range config.Shards {
		if _, ok := s2g[gid]; ok {
			s2g[gid] = append(s2g[gid], i)
		} else {
			s2g[gid] = []int{i}
		}
	}

	for gid := range config.Groups {
		if _, ok := s2g[gid]; !ok {
			s2g[gid] = []int{}
		}
	}

	return s2g
}
```

### leave
leave方法的参数是要剔除出去的group：`GIDs []int`

leave方法首选要做的是将group从newConfig.Groups中剔除，然后将它们的shard都存起来（这里变量名为orphanShards）
```go
//...
	newConfig := Config{len(sm.configs), lastConfig.Shards, deepCopy(lastConfig.Groups)}
	s2g := Group2Shards(newConfig)
	orphanShards := make([]int, 0)
	for _, gid := range gids {
		if _, ok := newConfig.Groups[gid]; ok {
			delete(newConfig.Groups, gid)
		}
		if shards, ok := s2g[gid]; ok {
			orphanShards = append(orphanShards, shards...)
			delete(s2g, gid)
		}
	}
//...
```
然后同样进行rebalance操作。这里每次都将待分配的shard分配给shard数最少的group
```go
//...
	var newShards [NShards]int
	if len(newConfig.Groups) != 0 {
		for _, shard := range orphanShards {
			target := GetGIDWithMinimumShards(s2g)
			s2g[target] = append(s2g[target], shard)
		}
		for gid, shards := range s2g {
			for _, shard := range shards {
				newShards[shard] = gid
			}
		}
	}
//...
```


### move 与 query
query方法只需要返回最后一个config。
move方法是做一次迁移操作
```go
func (sm *ShardMaster) doMove(shard int, gid int) Err {
	lastConfig := sm.configs[len(sm.configs)-1]
	newConfig := Config{len(sm.configs), lastConfig.Shards, deepCopy(lastConfig.Groups)}
	newConfig.Shards[shard] = gid
	sm.configs = append(sm.configs, newConfig)
	return OK
}
```


# Lab4B
Lab4B的系统会用到Lab4A的配置管理系统，用shardmaster来负责配置更新与分片分配。然后系统会分出多个raft组来承载所有分片的读写任务。在处理读写的过程中，系统要处理raft组的变更、节点宕机与重启、网络分区等各种情况。同样的，所有操作都要保证线性一致性。

**这个lab不能通过全部test，仅做思路参考。**

实现时，我先参考了作业要求信息的步骤，先实现一个不考虑分片变化的静态系统。这可以直接照搬lab2的代码，但是要注意的是，需要根据提供的算法来决定向哪个分片进行操作：

```go
func key2shard(key string) int {
	shard := 0
	if len(key) > 0 {
		shard = int(key[0])
	}
	shard %= shardmaster.NShards
	return shard
}
```

每个server有一个gid，表示自己属于哪个raft组。在每次接受到Get、Put、Append请求时，如果发现该server所在的组没有这个分片（根据lab3A的config信息判断），就返回`ErrWrongGroup`。做完这些之后，可以顺利得通过第一个测试任务`TestStaticShards`。

```text
# GO111MODULE=off go test -run TestStaticShards  

Test: static shards ...
  ... Passed
PASS
ok      .../MIT6.824/src/shardkv      6.189s
```

但是如果考虑到分片迁移应该怎么办呢？首先，根据课程提示，我们至少要有一个线程来拉取最新的配置，使自身的配置得到更新。其次，我们需要有一个rpc方法，帮助我们从一个raft组传递数据给另一个raft组。当然，这些操作也都必须经过raft层的应用。

有了以上的目标之后，先设计一下相关的数据结构：

Op是提交到raft中的结构，可以有为空的字段，但需要包含各种信息。参考了其他人的设计，Op中的CommandType分为Operation、Configuration、UpdateShards这几种。Operation就是客户端读写操作，Configuration是配置更新日志，UpdateShards是分片的数据更新操作。CommandType还可以加一个DeleteShards用来实现lab challenge目标中的gc任务，这里略过。
```go
type Op struct {
	CommandType CommandType

	Type      string // get put append
	Key       string
	Value     string
	ClientId  int64
	CommandId int

	Config *shardmaster.Config

	ShardOperationResponse *ShardOperationResponse
}
```

server的结构中加了这几个：
```go
//...
	sm            *shardmaster.Clerk  // 用来发送rpc
	lastConfig    shardmaster.Config  // 前一次配置
	currentConfig shardmaster.Config  // 最新配置

	persister     *raft.Persister
	shardDatabase map[int]*Shard // shard database
	prevOperation map[int64]CommandResponse
	notifyChanMap map[int]chan CommandResponse

	updating   bool  // 这两个与更新shard数据有关
	updatedNum int
//...
```

Shard结构保存了一个分片的状态和数据：
```go
type ShardStatus uint8

const (
	Serving ShardStatus = iota
	ToPull
	Pulling
	GCing
)

type Shard struct {
	KV     map[string]string
	Status ShardStatus
}

func NewShard() *Shard {
	return &Shard{make(map[string]string), Serving}
}
```

然后在启动方法中加入定时操作的线程：
```go
// 定时任务
func (kv *ShardKV) monitor(action func(), timeout time.Duration) {
	for kv.killed() == false {
		if _, isLeader := kv.rf.GetState(); isLeader {
			action()
		}
		time.Sleep(timeout)
	}
}

// start方法：
//...
	go kv.monitor(kv.configureAction, ConfigureMonitorTimeout)
	go kv.monitor(kv.migrationAction, MigrationMonitorTimeout)
//...
```

配置更新操作这两个方法，一个是configureAction，用来拉取配置，一个是applyConfiguration，应用配置：
```go
func (kv *ShardKV) configureAction() {
	canPerformNextConfig := true
	kv.mu.Lock()
	if kv.updating {
		canPerformNextConfig = false
	}
	currentConfigNum := kv.currentConfig.Num
	kv.mu.Unlock()
	if canPerformNextConfig {
		nextConfig := kv.sm.Query(currentConfigNum + 1)
		if nextConfig.Num == currentConfigNum+1 {
			//log(fmt.Sprintf("{Group %v}{Node %v} fetches latest configuration %v when currentConfigNum is %v", kv.gid, kv.me, nextConfig, currentConfigNum), 1)
			op := Op{CommandType: Configuration, Config: &nextConfig}
			kv.executeOp(op)
		}
	}
}

// 在raft中apply成功后调用这个方法
func (kv *ShardKV) applyConfiguration(nextConfig *shardmaster.Config) CommandResponse {
	if nextConfig.Num == kv.currentConfig.Num+1 {
		//log(fmt.Sprintf("{Group %v}{Node %v} updates currentConfig from %v to %v", kv.gid, kv.me, kv.currentConfig, nextConfig), 2)
		kv.lastConfig = kv.currentConfig
		kv.currentConfig = *nextConfig
		return CommandResponse{Err: OK}
	}
	//log(fmt.Sprintf("{Group %v}{Node %v} rejects outdated config %v when currentConfig is %v", kv.gid, kv.me, nextConfig, kv.currentConfig), 1)
	return CommandResponse{Err: ErrOutDated}
}
```

迁移shard数据的方法也是类似的，一个是migrationAction，另一个是applyUpdateShards。另外还有一个GetShardsData方法为rpc调用。

migrationAction会先通过调用getToPullShardIdMap，根据前一次配置和当前配置去生成更新清单，指向哪些raft组请求哪些raft分片的信息。然后进行分片迁移操作。注意我这里通过设置updating的方式阻塞了读写操作与配置更新操作，这样不符合challenge目标，可以后续通过shard状态来优化。
```go
func (kv *ShardKV) migrationAction() {
	kv.mu.Lock()
	if kv.updating {
		kv.mu.Unlock()
		return
	}
	gid2shardIDs := kv.getToPullShardIdMap()
	if len(gid2shardIDs) > 0 {
		log(fmt.Sprintf("{Group %v}{Node %v} migrationAction gid2shardIDs:%+v", kv.gid, kv.me, gid2shardIDs), 2)
	} else {
		kv.mu.Unlock()
		return
	}
	kv.updating = true
	var wg sync.WaitGroup
	for gid, shardIDs := range gid2shardIDs {
		log(fmt.Sprintf("{Group %v}{Node %v} starts a PullTask to get shards %v from group %v when config is %v", kv.gid, kv.me, shardIDs, gid, kv.currentConfig), 2)
		wg.Add(1)
		go func(servers []string, configNum int, shardIDs []int) {
			defer wg.Done()
			pullTaskRequest := ShardOperationRequest{configNum, shardIDs}
			for _, server := range servers {
				var pullTaskResponse ShardOperationResponse
				srv := kv.make_end(server)
				keepTry := true
				for keepTry {
					srv.Call("ShardKV.GetShardsData", &pullTaskRequest, &pullTaskResponse)
					if pullTaskResponse.Err == OK {
						log(fmt.Sprintf("{Group %v}{Node %v}gets a PullTaskResponse %+v and tries to commit it when currentConfigNum is %v", kv.gid, kv.me, pullTaskResponse, configNum), 2)
						op := Op{CommandType: UpdateShards, ShardOperationResponse: &pullTaskResponse}
						kv.executeOp(op)
						keepTry = false
					} else if pullTaskResponse.Err == ErrWrongLeader {
						keepTry = false
					} else {
						log(fmt.Sprintf("{Group %v}{Node %v}gets a PullTaskResponse %+v currentConfigNum is %v", kv.gid, kv.me, pullTaskResponse, configNum), 2)
						time.Sleep(time.Duration(100) * time.Millisecond)
					}
				}
			}
		}(kv.lastConfig.Groups[gid], kv.currentConfig.Num, shardIDs)
	}
	kv.mu.Unlock()

	wg.Wait()

	kv.mu.Lock()
	kv.updating = false
	kv.updatedNum = kv.currentConfig.Num
	kv.mu.Unlock()
}

func (kv *ShardKV) getToPullShardIdMap() map[int][]int {
	gid2ShardIds := make(map[int][]int)
	if kv.updatedNum == kv.currentConfig.Num {
		return gid2ShardIds
	}
	for shardId, oldGid := range kv.lastConfig.Shards {
		if oldGid == 0 {
			continue // init
		}
		if kv.shardDatabase[shardId].Status == ToPull {
			continue
		}
		newGid := kv.currentConfig.Shards[shardId]
		if newGid == kv.gid && newGid != oldGid {
			for _, gId := range kv.lastConfig.Shards {
				if gId == oldGid {
					gid2ShardIds[gId] = append(gid2ShardIds[gId], shardId)
					kv.shardDatabase[shardId].Status = ToPull
					break
				}
			}
		}
	}
	return gid2ShardIds
}
```

在迁移过程中会新开线程做拉取配置操作与向raft提交操作记录
```go
func (kv *ShardKV) GetShardsData(request *ShardOperationRequest, response *ShardOperationResponse) {
	// only pull shards from leader
	if _, isLeader := kv.rf.GetState(); !isLeader {
		response.Err = ErrWrongLeader
		return
	}
	kv.mu.Lock()
	//defer log(fmt.Sprintf("{Group %v}{Node %v} processes PullTaskRequest %+v with response %+v", kv.gid, kv.me, *request, *response), 2)
	defer kv.mu.Unlock()

	if kv.currentConfig.Num < request.ConfigNum {
		log(fmt.Sprintf("{Group %v}{Node %v} processes PullTaskRequest %+v ErrOutDated: kv.currentConfig.Num %v, request.ConfigNum %v", kv.gid, kv.me, *request, kv.currentConfig.Num, request.ConfigNum), 2)
		response.Err = ErrOutDated
		return
	}

	response.ShardData = make(map[int]map[string]string)
	for _, shardId := range request.ShardIds {
		shard := kv.shardDatabase[shardId]
		response.ShardData[shardId] = shard.deepCopy()
	}

	response.PrevOperation = make(map[int64]CommandResponse)
	for clientID, operation := range kv.prevOperation {
		response.PrevOperation[clientID] = operation.deepCopy()
	}

	response.ConfigNum, response.Err = request.ConfigNum, OK
}
```

最后是应用shard迁移的数据。
```go
func (kv *ShardKV) applyUpdateShards(shardsInfo *ShardOperationResponse) CommandResponse {
	if shardsInfo.ConfigNum == kv.currentConfig.Num {
		log(fmt.Sprintf("{Group %v}{Node %v} accepts shards insertion %v when currentConfig is %v", kv.gid, kv.me, shardsInfo, kv.currentConfig), 1)
		for shardId, shardData := range shardsInfo.ShardData {
			shard := kv.shardDatabase[shardId]
			for key, value := range shardData {
				shard.KV[key] = value
			}
			shard.Status = Serving
		}
		for clientId, commandResponse := range shardsInfo.PrevOperation {
			kv.prevOperation[clientId] = commandResponse
		}
		return CommandResponse{Err: OK}
	}
	log(fmt.Sprintf("{Node %v}{Group %v} rejects outdated shards insertion %v when currentConfig is %v", kv.me, kv.gid, shardsInfo, kv.currentConfig), 1)
	return CommandResponse{Err: ErrOutDated}
}
```

这样实现后，可以顺利通过测试2`TestJoinLeave`。

```text
# GO111MODULE=off go test -run TestJoinLeave     

  ... Passed
PASS
ok      .../MIT6.824/src/shardkv      6.069s
```

但是在测试3`TestSnapshot`时遇到了问题。宕机与恢复至少需要实现takeSnapshot与restoreSnapshot这两步，在实现了之后，系统的数据问题与死锁问题频发，表现为数据不符合预期或陷入死锁。对这步的调式异常痛苦。然后我重新思考了一下这两个方法的实现：
```go
// 代码块1 apply线程中发起snapshot请求的入口
if isLeader && message.CommandTerm == currentTerm {
	ch := kv.getNotifyChan(message.CommandIndex)
	if ch != nil {
		ch <- response
	}
	if kv.needSnapshot() {
		kv.takeSnapshot(message.CommandIndex)
	}
}

// 代码块2 apply线程中应用snapshot的入口
if message.SnapshotValid {
	kv.mu.Lock()
	kv.restoreSnapshot(message.Snapshot)
	kv.mu.Unlock()
}


func (kv *ShardKV) takeSnapshot(index int) {
	buffer := new(bytes.Buffer)
	encoder := labgob.NewEncoder(buffer)
	encoder.Encode(kv.shardDatabase)
	encoder.Encode(kv.prevOperation)
	encoder.Encode(kv.lastConfig)
	encoder.Encode(kv.currentConfig)
	encoder.Encode(kv.updating)
	encoder.Encode(kv.updatedNum)
	log(fmt.Sprintf("{Group %v}{Node %v} 创建快照数据:index %v shardDatabase %+v", kv.gid, kv.me, index, printShardDatabase(kv.shardDatabase)), 3)
	kv.rf.TakeSnapshot(index, buffer.Bytes())
}

// 从快照中读取数据，并且将当前的db和prevOperation替换
func (kv *ShardKV) restoreSnapshot(snapshot []byte) {
	if snapshot == nil || len(snapshot) == 0 {
		return
	}
	buffer := bytes.NewBuffer(snapshot)
	decoder := labgob.NewDecoder(buffer)
	var db *map[int]*Shard
	var prevOperation *map[int64]CommandResponse
	var lastConfig *shardmaster.Config
	var currentConfig *shardmaster.Config
	var updating *bool
	var updatedNum *int

	if decoder.Decode(&db) != nil ||
		decoder.Decode(&prevOperation) != nil ||
		decoder.Decode(&lastConfig) != nil ||
		decoder.Decode(&currentConfig) != nil ||
		decoder.Decode(&updating) != nil ||
		decoder.Decode(&updatedNum) != nil {
		log(fmt.Sprintf("{Group %v}{Node %v} restore snapshot failed", kv.gid, kv.me), 3)
	}
	kv.shardDatabase = *db
	kv.prevOperation = *prevOperation
	kv.lastConfig = *lastConfig
	kv.currentConfig = *currentConfig
	kv.updating = *updating
	kv.updatedNum = *updatedNum
	log(fmt.Sprintf("{Group %v}{Node %v} 恢复快照数据: shardDatabase %+v, prevOperation %+v", kv.gid, kv.me, printShardDatabase(kv.shardDatabase), kv.prevOperation), 3)
}
```

然后回过头去看`migrationAction`方法，会发现中间是不能一个锁全锁到底的，必须释放锁等待网络请求再更新。那么在锁的重新获取期间，如果发生了snapshot操作，会将更新半途的数据都存进去，再恢复时，由于无法再回到原来方法的进度中，所以恢复后的数据和迁移的数据是很有可能“打架”的。

因此作出调整，不再持久化updating和updatedNum，而且，恢复数据后，默认所有的shard都处于需要更新的阶段，以防止漏更新：
```go
for _, shard := range kv.shardDatabase {
	shard.Status = ToPull
}
```

这样改了之后就能通过第三个test了
```text
# GO111MODULE=off go test -run TestSnapshot    

Test: snapshots, join, and leave ...
labgob warning: Decoding into a non-default variable/field Err may not work
  ... Passed
PASS
ok      _/home/yuic/MyProjects/golang/MIT6.824/src/shardkv      17.516s
```

不过后面的几个test依然会遇到的问题依然有死锁等待和返回值有误的情况。其中一个问题点大致定位在updating阻塞这套方案上，这在异常网络条件下会导致两个Raft组死锁等待分片数据。

另外2个challenge也需要进行优化。如需要在迁移过程中只阻塞待更新的shard，这需要对shard的状态有更清晰的定义与逻辑实现。另外还需要一个gc方法，具体场景为，在迁移走数据shard数据后，raft组会清空已不需要的数据。




