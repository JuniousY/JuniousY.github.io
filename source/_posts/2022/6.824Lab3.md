---
title: MIT 6.824 Lab3
date: 2022-10-20 23:59:59
categories: Lab
tags: 
- distributed system
---

Lab3的目标是利用Raft的机制，实现一个线性一致（Linearizability）的key-value存储结构。

# Lab3A

### 概述
通过测试
```
Test: one client (3A) ...
labgob warning: Decoding into a non-default variable/field Err may not work
  ... Passed --  15.2  5  7307  289
Test: many clients (3A) ...
  ... Passed --  15.9  5 12944 1483
Test: unreliable net, many clients (3A) ...
  ... Passed --  16.1  5  3911 1029
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   1.2  3   163   52
Test: progress in majority (3A) ...
  ... Passed --   1.2  5   167    2
Test: no progress in minority (3A) ...
  ... Passed --   1.1  5   191    3
Test: completion after heal (3A) ...
  ... Passed --   1.1  5    96    3
Test: partitions, one client (3A) ...
  ... Passed --  22.4  5 15465  175
Test: partitions, many clients (3A) ...
  ... Passed --  23.2  5 23550 1158
Test: restarts, one client (3A) ...
  ... Passed --  21.3  5 21560  289
Test: restarts, many clients (3A) ...
  ... Passed --  23.3  5 105143 1445
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  23.6  5  5069 1145
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  29.1  5 78246  825
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  32.1  5  4443  507
Test: unreliable net, restarts, partitions, many clients, linearizability checks (3A) ...
  ... Passed --  28.8  7  9497  887
PASS
```

一开始看到Lab3A的时候完全不知道从何入手，仔细看了下的话，其实kv server分为三个部分：Client, Server, Raft。其中Raft是Server的底层，是分布式Server做到线性一致的基础。这其中的通信关系是：Client <=> Server, Server <=> Raft, Raft <=> Raft。有两个重要的点，一个是Server和Server之间是不通信的，必须通过Raft通信，第二个是Server和Raft是一对一的关系，只有Raft leader对应的Server才有操作的资格。另外有一个细节是，在这个lab里，每个Get请求也必须通过raft协议才能完成，这样做的目的是保证线性一致。

### Client
首先从Cliet处着手。保证消息不重复消费最重要的一部分是Client会有自己的clientId，每次指令也有一个commandId。Client的Get和Put方法就是循环调用server的接口，如果不成功就换下一个server，直到server返回成功。

```go
for {
  var getArgs GetArgs = GetArgs{
    Key:       key,
    ClientId:  ck.clientId,
    CommandId: ck.commandId,
  }
  var getReply GetReply
  for {
    ok := ck.servers[ck.leaderId].Call("KVServer.Get", &getArgs, &getReply)
    if !ok || getReply.Err == ErrWrongLeader || getReply.Err == ErrTimeout {
      ck.leaderId = (ck.leaderId + 1) % (len(ck.servers))
      continue
    }
    DPrintf("###client %v Get ok %#v, commandId:'%v' \n", ck.clientId, getReply, ck.commandId)
    ck.commandId++
    return getReply.Value
  }
}
```

### Server
Server端会比较复杂。Server和Client通信是用的RPC，那么Server和Raft是怎么通信的呢？答案是Server向Raft发起请求是通过Raft的Start()入口方法，然后通过Raft的applyCh这个channel来获取数据。因此在Server新建的时候，需要新开一个线程用来获取applyCh的数据。

Start方法传入的数据结构会存到Raft的日志中：
```go
type Op struct {
	Type      string // get put append
	Key       string
	Value     string
	ClientId  int64
	CommandId int
}
```

Server在收到Get、Put、Append的RPC请求时，要做这几件事：
1. 构造Op参数
2. 根据ClientId和CommandId判断是不是重复请求
3. 调用Raft的Start方法
4. 收到Start方法结果，如果返回值中isLeader是false，就返回ErrWrongLeader给Client
5. 生成一个channel，在applyCh数据返回后给 apply message 线程后，线程会把数据传回到这个channel。接收成功就可以返回给client结果。

这个channel需要做超时处理：

```go
DPrintf("leader %v 开始等待PutAppend结果: index %v, isLeader %v, args %+v", kv.me, index, isLeader, args)
ch := kv.makeNotifyChan(index)
kv.mu.Unlock()
select {
case <-time.After(ExecuteTimeout):
  DPrintf("leader %v PutAppend 超时: index %v, isLeader %v, args %+v", kv.me, index, isLeader, args)
  reply.Err = ErrTimeout
case <-ch:
  reply.Err = OK
  DPrintf("leader %v PutAppend结果得到: index %v, isLeader %v, args %+v, reply %+v", kv.me, index, isLeader, args, reply)
}
go func() { kv.closeNotifyChan(index) }()
```

apply message 线程就是循环读取applyCh传来的数据，如果是写入操作，就把操作写入server的数据库中。在本lab里，这个操作是写入到一个map里。这里有一个重要的点，是判断applyCh传来的数据是不是过时的数据。因此，我的做法是在server中维护一个结构`prevOperation map[int64]CommandResponse`，记录每个Client上一次返回结果。CommandResponse中有CommandId信息。如果applyCh传来的数据中，它的CommandId小于等于前一次返回结果的CommandId，就直接抛弃这条消息。最后server把数据传回给rpc请求：
```go
ch := kv.getNotifyChan(message.CommandIndex)
if ch != nil {
  ch <- response
}
```
这里有个细节是如果拿不到channel了，就说明rpc方法已经超时返回了，删除了这个channel。这里的处理是试了很多次之后定下来的，能通过测试，但可能还是有点疑问。因为如果这里不传消息，其实是相当于丢了一次已经记录到Raft中的消息，而如果不传，那这里因为rpc已经返回了，所以channel会永远在等待发送消息中。参考了一下其他人的实现，有的人会说这里不会阻塞，可能是实现的一些细节不同吧。

整个LAB3A做下来的感受其实是挺折磨的，最大的问题是很容易遇到死锁问题，需要写大量的log，尤其是在加锁解锁和channel通信处。下面举几个例子：

死锁一是在上面说的阻塞的情况时发生：
```go
// putappend
DPrintf("putAppend 0")
kv.mu.Lock()
DPrintf("putAppend 1")
// dosomething
kv.mu.Unlock()

// apply线程
kv.mu.Lock()
...
DPrintf("apply 1")
ch <- response
DPrintf("apply 2")
kv.mu.Unlock()
DPrintf("apply 3")
```

```text
// 输出结果
apply 1
putAppend 1
putAppend 0
```
由于apply处有锁，而apply阻塞等待消息，因此rpc方法处的锁无法拿到。这里的错误点在于发送消息时应该是没有锁的。要么提前解锁，要么新开一个线程发送消息。


死锁二，apply必须在是leader情况下时才能发送管道消息，不然就永远在等待。为什么呢？因为只有在leader情况下才会新建chanel。当然上面判断channel是不是为空也是一种解决方法，但更正确的做法应该是只在是leader而且term相符情况下才发送消息：
```go
currentTerm, isLeader := kv.rf.GetState()
if isLeader && message.CommandTerm == currentTerm {
  // send
}
```

死锁三是在raft中发生的，这算是之前lab2中一个没有暴露出来的bug。
之前在raft发送applyCh消息时，是持有raft的锁的。当apply一次性提交很多个数据时，会一直占用rf.mu。但是在Server处，会调用rf.Start()或者rf.GetState()，这两个都要求锁。于是，Server端等待rf的锁，无法处理applyCh的下一条消息，而raft持有锁，等待向applyCh中发送消息，于是引发了死锁。
解决方法和上面一样，在消息发送时，要么解锁，要么新开线程：

```go
// 问题代码：

// server层
currentTerm, isLeader := kv.rf.GetState()

// raft层
//  加锁状态 一次有多个msg发送
rf.applyCh <- msg
```

```go
// 需要改成：

rf.mu.Unlock()
rf.applyCh <- msg
rf.mu.Lock()
```


# Lab3B

```
Test: InstallSnapshot RPC (3B) ...
  ... Passed --   7.5  3  4540   63
Test: snapshot size is reasonable (3B) ...
  ... Passed --  41.2  3 10828  800
Test: restarts, snapshots, one client (3B) ...
  ... Passed --  20.8  5 17973  289
Test: restarts, snapshots, many clients (3B) ...
  ... Passed --  25.3  5 129392 5900
Test: unreliable net, snapshots, many clients (3B) ...
  ... Passed --  16.0  5  3476  848
Test: unreliable net, restarts, snapshots, many clients (3B) ...
  ... Passed --  23.5  5  4381  730
Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ...
... Passed --  32.0  5  3448  343

```
Lab3B要做的事情很简单，就是将日志压缩为snapshop。但实际我自己做下来比lab3A要繁琐得多，也尝试了很久。而且虽然通过了所有的测试，但最后三个测试有小概率失败，也很难定位到问题。


具体来说，Lab3B的流程是，server在发现日志大小超过某个临界值之后，将自己的数据序列化存储为快照，然后传给raft。raft会删除旧的日志，保留必要的信息。

### Server层
server比较简单，但也会有坑。首先在apply线程中，在拿到raft返回数据后，判断要不要生成快照：
```go
func (kv *KVServer) needSnapshot() bool {
	if kv.maxraftstate == -1 {
		return false
	}
	return kv.persister.RaftStateSize() > kv.maxraftstate
}
```

如果需要，就要讲自己的数据map、lastAppliedIndex表和prevOperation表序列化生成快照。后面两个在恢复server的时候是非常有必要的，防止多次提交。在序列化的最后，是调取一个新的Raft方法传递当前的日志index（很重要）和快照数据`kv.rf.TakeSnapshot(index, buffer.Bytes())`。

反过来也需要一个反序列化的读取方法，将这些数据应用到Server层中。触发的时间点是Raft通过applyCh将操作成功的消息传递给Server层时。一个细节是启动时候也要进行这一步。

### Raft层
首先，Raft层接受Server命令的方法会进行删除日志、记录快照最后包含的Index。Raft层会新增两个字段lastIncludedIndex、lastIncludedTerm。然后leader将快照数据持久化。

然后，Raft层会多一个RPC请求，这是leader向follower发送snapshot命令的请求：
```go
func (rf *Raft) InstallSnapshot(args *InstallSnapshotArgs, reply *InstallSnapshotReply) {
  // ...  省略判断校验逻辑
  logs := make([]Log, 0)
  startIndex := rf.newIndex(args.LastIncludedIndex + 1)
  if startIndex <= len(rf.logEntries) {
    logs = append(logs, rf.logEntries[startIndex:]...)
  }
  rf.logEntries = logs

  rf.lastIncludedIndex = args.LastIncludedIndex
  rf.lastIncludedTerm = args.LastIncludedTerm

  rf.lastApplied = max(rf.lastIncludedIndex, rf.lastApplied)
  rf.commitIndex = max(rf.lastIncludedIndex, rf.commitIndex)

  // ... 省略持久化数据

  // ... 省略发送消息给Server层
}
```
这个核心方法是删除日志，然后将相关index修改。

那么，什么时候leader向server发送这一消息呢？我的处理是这样的，每次leader持久化之后就对所有follower进行一次发送。然后，在发送AppendEntries处，如果nextIndex的值已经和日志不匹配了，就说明也要发送。这种情况是follower脱离集群之后再回来会发生的事：
```go
prevLogIndex := rf.nextIndex[id] - 1
if rf.newIndex(prevLogIndex) >= len(rf.logEntries) || rf.newIndex(prevLogIndex) < -1 {
  //fmt.Printf("#### rf.me %v rf.nextIndex[%v] = %v, rf.lastIncludedIndex %v\n", rf.me, id, rf.nextIndex[id], rf.lastIncludedIndex)
  rf.mu.Unlock()
  rf.sendSnapshot(id)
  return
}
```

这里有一个非常头疼的地方是，下标index怎么处理。日志list是新建的，原来的下标是对应不上的。有两种思路，一种是，在新建日志时，重算所有的下标并更新；另一种思路是，所有的下标保持原样，在使用到日志list上时做处理。
一开始，我在尝试了一下第二种思路后，选择放弃了，因为要改的地方很多，以为前一种思路更简单一点，因为只要改一处地方。但实际上试下来非常头疼，debug了很久，因为很容易出现旧下标和新下标混用的情况。具体遇到的问题如：1. Server层在get方法时，apply线程将信息发给了别的rpc接口。因为获取channel的参数是Start方法的返回值中的index，一旦index变化，就会发错数据。2. 在解决了问题一之后，Raft层的数据有概率出现错配的情况，也很难调试出原因。
然后我重看了一下论文，再参考了一下别人的实现，思考了一下之后我觉得只有第二种才是正常的实现，因为各种index（如applied index）必须是递增的，没有理由去改动它们。所以，必须在读取日志list时做改动：
```go
func (rf *Raft) newIndex(index int) int {
	return index - rf.lastIncludedIndex - 1
}

func (rf *Raft) trueIndex(index int) int {
	return index + rf.lastIncludedIndex + 1
}
```
这里面头疼的点是，有些不是直接和list下标有关的方法内部参数，也需要改动，举一个例子，选举时有一个参数`lastLogIndex`，就需要算成实际的下标来使用，否则出现的情况是，一个没有数据的新follower会因为term较高而被选举为leader。

解决为下标问题，Lab3B就没有什么大的问题了。总的来说，Lab3需要谨慎规避死锁问题，需要在多线程分布式环境下打充足的日志来调试问题。功能点上可能不是很多，但是很能锻炼工程实现能力和复杂问题处理能力
