---
title: MIT 6.824 Lab2B
date: 2022-08-09 01:01:45
categories: Lab
tags: 
- distributed system
---

这部分是Raft的核心，先上通过记录。

```text
GO111MODULE=off go test -run 2B

Test (2B): basic agreement ...
  ... Passed --   1.1  3   16    4688    3
Test (2B): RPC byte count ...
  ... Passed --   2.8  3   48  114908   11
Test (2B): agreement despite follower disconnection ...
  ... Passed --   6.7  3  124   33767    8
Test (2B): no agreement if too many followers disconnect ...
  ... Passed --   4.2  5  212   42984    3
Test (2B): concurrent Start()s ...
  ... Passed --   0.8  3   10    2890    6
Test (2B): rejoin of partitioned leader ...
  ... Passed --   6.9  3  172   43180    4
Test (2B): leader backs up quickly over incorrect follower logs ...
  ... Passed --  28.6  5 2308 1695419  103
Test (2B): RPC counts aren't too high ...
  ... Passed --   5.8  3  100   29488   12
PASS
ok      ../MIT6.824/src/raft 56.896s
```

一开始进行任务拆分。实际上除了任务1和2，对其他任务是一起推进实现的，因为很多逻辑都是上下游的关系。
1. 实现Start方法，实现leader自己更新记录
2. server通过applyCh返回结果
3. leader向follower发送AppendEntries
4. follower接收处理AppendEntries
5. leader得到follower返回结果（或返回失败）时的处理
6. 确保选举成功的candidate包含所有已提交的记录（实现election restriction）


## 实现细节

### 1 Start方法和Apply方法

Start方法就是业务层向Raft层提交任务的唯一入口。Leader在这个方法里添加log `rf.logEntries = append(rf.logEntries, Log{rf.currentTerm, command})`。有个小细节是，可以在此处设置 `rf.matchIndex[rf.me] = index`，index为最后一个元素的index。这样做的好处是在后续计算能否commit记录时好处理一些。

Apply方法就是Raft层向业务层返回结果的唯一出口。
```go
type ApplyMsg struct {
	CommandValid bool
	Command      interface{}
	CommandIndex int
}

type Raft struct {
    //...
    applyCh         chan ApplyMsg
    //...
}

func (rf *Raft) startApplyLogs() {
	for rf.lastApplied < rf.commitIndex {
		msg := ApplyMsg{}
		rf.lastApplied++
		msg.CommandIndex = rf.lastApplied
		msg.Command = rf.logEntries[rf.lastApplied].Command
		msg.CommandValid = true
		//DPrintf("server %d: 开始应用log, commitIndex: %d, lastApplied: %d, msg %v ", rf.me, rf.commitIndex, rf.lastApplied, msg)
		rf.applyCh <- msg
	}
}
```

### 2 AppendEntries
server在接受AppendEntries时，至少要有以下校验逻辑：
1. 如果args.PrevLogIndex比自身log最后一个元素的index还要大，就返回`ConflictIndex=len(rf.logEntries)`
2. 如果args.PrevLogTerm和自身的PrevLogTerm不同，返回`ConflictTerm = thisPrevLogTerm`、`ConflictIndex`为该Term下的第一个index

普通server的commitIndex更新逻辑：
```go
	if args.LeaderCommit > rf.commitIndex {
		rf.commitIndex = int(math.Min(float64(args.LeaderCommit), float64(len(rf.logEntries)-1)))
	}
```

leader在发送AppendEntries后，要对server返回结果进行处理，至少要对以下异常情况进行校验：
1. 没有收到RPC结果，则不进行后续逻辑
2. term小于返回值的term，则变为leader
3. ConflictTerm或ConflictIndex有值时，重设`nextIndex[id] //id为server的id`，返回，等待下一次发送时从冲突处发送记录

最后实现Figure 2中的这条：如果存在 N，N > commitIndex，大部分matchIndex[i] >= N，log[N].term == currentTerm，那么，就设置commitIndex = N。Leader的commitIndex在此处更新。


### 3 election restriction
按照up-to-date的定义，follower在vote时要判断该布尔值：`upToDate := args.LastLogTerm < lastLogTerm || (args.LastLogTerm == lastLogTerm && args.LastLogIndex < lastLogIndex)`
如果为true，说明follower不该向这个candidate投出这票，因为follower相对这个candidate更有资格成为leader。这个判断很关键，后续问题点3这个例子就是不判断时会出现的情况。



## 实现中遇到的问题

### 问题点1 - TestBasicAgree2B
TestBasicAgree2B 偶尔会无法通过。 

多试几次后发现，原因是 leader还未发送AppendEntries就有新的candidate出现并成功选举为leader。而前leader并没有丢弃自身的log，导致不一致。
``` text
2022/08/06 21:46:22 @@@ Leader 2: got a new Start task, command: 100
2022/08/06 21:46:22 server 1 成为 candidate, currentTerm 2
2022/08/06 21:46:22 === Candidate 1 开始发送 RequestVote, currentTerm 2 ===
2022/08/06 21:46:22 server 2 (term 1 voteFor 2) 收到 candidiate 1 (term 2 candidateId 1) 的RequestVote
2022/08/06 21:46:22 server 2 成为 follower，currentTerm 1 ==> 2, leader id 2 ==> -1
2022/08/06 21:46:22 server 0 (term 1 voteFor 2) 收到 candidiate 1 (term 2 candidateId 1) 的RequestVote
2022/08/06 21:46:22 server 0 投票给 server 1
2022/08/06 21:46:22 *** server 1 成为 leader, currentTerm 2 ***
2022/08/06 21:46:22 === server 0 处理 Leader 1 的 AppendEntries 成功，当前logEntries [] ===
2022/08/06 21:46:22 === server 2 处理 Leader 1 的 AppendEntries 成功，当前logEntries [{1 100}] ===

```

一方面 server 要放弃之后的内容
```go
...
	endIndex := args.PrevLogIndex + len(args.Entries) + 1
	if endIndex < len(rf.logEntries) {
		rf.logEntries = rf.logEntries[:endIndex]
	}
...
```

另一方面，根据election restriction机制，一个candidate必须包含所有已提交的entries。具体实现方式是：投票这如果发现自己的log比candidate更新，则不投票。
更（第四声）新（up-to-date）的含义是，如果末尾entries的term更大则更新，如果term一样，则log长度长的算更新。具体实现见上文。

在本例中，尽管实现了这一机制，但是3个server中，哪怕之前收到log的前leader2没有投票，server1 有自己一票和server0 一票，依然能当选。

再仔细想一下，这样的结果其实是正确的，因为该entry确实没有被提交。但TestBasicAgree2B是要求每次请求都成功写入的（毕竟确实没有异常出现）。究其原因，最大的问题还是出在选举时，server2明明已经选上leader了，结果server1没有收到心跳。

再检查一下代码：

```go
func (rf *Raft) candidateMainFlow() {
	... 
	rf.startRequestVote() // line a 此处会成为leader 
	time.Sleep(time.Duration(electionTimeout) * time.Millisecond)  // line b 此处会等待
	rf.mu.Lock()
	if rf.state == CANDIDATE && !rf.heartBeat {
		rf.convertToCandidate()
	}
	rf.mu.Unlock()
}
```
成为leader后没有立刻发送心跳，反而进行了一次等待。所以应该在上述代码中的line a 和 line b中间发送一次空的AppendEntries
```go
	rf.mu.Lock()
	isLeader := rf.state == LEADER
	rf.mu.Unlock()
	if isLeader {
		rf.startAppendEntries()
		return
	}
```

但是这样还是会出错。检查发现测试程序一但发现有leader产生之后就会写命令。所以最后引入了firstHeartBeat这个布尔值，在第一次心跳发送前，不允许接受命令，也就是Start方法处的返回结果isLeader是false。这实际是个非标准的做法，不过确实有用。

单改这个地方还不够，另一个测试如果不能在发现有leader后立刻能够插入数据也会报错，所以GetState处的结果也不能立刻返回是不是leader，最后加了一下比较tricky的解决：
```go
func (rf *Raft) GetState() (int, bool) {

	var term int
	var isleader bool
	// Your code here (2A).
	rf.mu.Lock()
	term = rf.currentTerm
	isleader = rf.state == LEADER
	for isleader && !rf.firstHeartBeat {
		rf.mu.Unlock()
		time.Sleep(time.Duration(5) * time.Millisecond)
		rf.mu.Lock()
		isleader = rf.state == LEADER
	}
	rf.mu.Unlock()
	return term, isleader
}
```

### 问题点2 - TestFailNoAgree2B
TestFailNoAgree2B 会在5个server中让3个server（不包含原leader）离线，然后进行操作。具体出现问题的测试代码为：
```go
// 3 of 5 followers disconnect
	leader := cfg.checkOneLeader()
	cfg.disconnect((leader + 1) % servers)
	cfg.disconnect((leader + 2) % servers)
	cfg.disconnect((leader + 3) % servers)
	
	index, _, ok := cfg.rafts[leader].Start(20)
	if ok != true {
		t.Fatalf("leader rejected Start()")
	}
	if index != 2 {
		t.Fatalf("expected index 2, got %v", index)
```

这时候由于半数宕机，理论上命令不会被写入。不过在我一开始的实现中，没有离线的两个server依然会写入，这样就有问题。

检查代码发现是applyIndex和commitIndex的赋值存在bug，解决后通过。


### 问题点3 - TestRejoin2B
TestRejoin2B 是这么操作的：
```text
leader1 写入 101
leader1 离线
leader1 写入 102 103 104
leader2 （新的leader） 写入 103
leader2 离线
leader1 上线
leader1 写入104
leader2 上线
写入 105
```

遇到的问题是leader1重新上线后无法选举出新的leader。如下面的例子，server2 和 server 0互相都不投给对方vote
```text
connect(2)
2022/08/07 00:41:56 @@@ Leader 2: got a new Start task, command: 104
2022/08/07 00:41:56 Leader 2 发送给 server 1 AppendEntries，args: {Term:1 LeaderId:2 PrevLogIndex:1 PrevLogTerm:1 Entries:[{Term:1 Command:102} {Term:1 Command:103} {Term:1 Command:104} {Term:1 Command:104}] LeaderCommit:1}
2022/08/07 00:41:56 Leader 2 发送给 server 0 AppendEntries，args: {Term:1 LeaderId:2 PrevLogIndex:1 PrevLogTerm:1 Entries:[{Term:1 Command:102} {Term:1 Command:103} {Term:1 Command:104} {Term:1 Command:104}] LeaderCommit:1}
2022/08/07 00:41:56 server 2 成为 follower，currentTerm 1 ==> 2, leader id 2 ==> -1
2022/08/07 00:41:57 server 0 成为 candidate, currentTerm 3
2022/08/07 00:41:57 === Candidate 0 开始发送 RequestVote, currentTerm 3 ===
2022/08/07 00:41:57 server 2 (term 2 voteFor -1) 收到 candidiate 0 (term 3 candidateId 0) 的RequestVote
2022/08/07 00:41:57 server 2 成为 candidate, currentTerm 4
2022/08/07 00:41:57 === Candidate 2 开始发送 RequestVote, currentTerm 4 ===
2022/08/07 00:41:57 server 0 (term 3 voteFor 0) 收到 candidiate 2 (term 4 candidateId 2) 的RequestVote
2022/08/07 00:41:57 server 0 成为 follower，currentTerm 3 ==> 4, leader id 0 ==> -1
2022/08/07 00:41:57 server 2 成为 candidate, currentTerm 5
```
这个问题是由于vote时候判断up-to-date的逻辑有问题导致的，上面已经讲过。修改后这个问题解决。


