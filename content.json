{"meta":{"title":"JuniousY的博客","subtitle":"倾听Ghost的低语","description":"后端开发,请多多指教","author":"JuniousY","url":"https://juniousy.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2022-05-30T09:06:50.975Z","updated":"2022-05-30T09:06:50.975Z","comments":false,"path":"/404.html","permalink":"https://juniousy.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2022-05-30T09:06:50.987Z","updated":"2022-05-30T09:06:50.987Z","comments":false,"path":"about/index.html","permalink":"https://juniousy.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"标签","date":"2022-05-30T09:06:50.993Z","updated":"2022-05-30T09:06:50.993Z","comments":false,"path":"tags/index.html","permalink":"https://juniousy.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-05-30T09:06:50.989Z","updated":"2022-05-30T09:06:50.989Z","comments":false,"path":"categories/index.html","permalink":"https://juniousy.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"MIT 6.824 Lab4","slug":"2023/6.824Lab4","date":"2023-05-05T15:59:59.000Z","updated":"2023-06-13T17:31:33.916Z","comments":true,"path":"2023/05/05/2023/6.824Lab4/","link":"","permalink":"https://juniousy.github.io/2023/05/05/2023/6.824Lab4/","excerpt":"","text":"Lab4的任务是在Lab2的基础上实现一个可迁移可配置的多节点KV存储系统。Lab4的实现难度很高，尤其是动态迁移数据这部分，实现与调试阶段都很痛苦，很难找出是哪一步导致没法实现线性一致性。整个设计对分布式系统的认识与设计能力都有很高的要求。这里参考了很多这篇文章的思路：https://github.com/OneSizeFitsQuorum/MIT6.824-2021/blob/master/docs/lab4.md#%E5%88%86%E7%89%87%E7%BB%93%E6%9E%84。也参考了一下这篇文章中的一些思路：https://sworduo.github.io/2019/08/16/MIT6-824-lab4-shardKV/。 Lab4ALab4A的任务目标是在Lab2的基础上实现一个高可用配置中心shardmaster。最核心的部分是通过raft实现这四个rpc方法： Join：向不同的组中添加server，然后产生一个新的config，使shard尽可能平均分配给各个组，且移动的shard最少 Leave：移除某一组，重分配它们的shard。同样需要使shard尽可能平均 Move：指定将某一个shard移动到某一个组 Query：返回最新的配置 这四个方法对应的配置数据结构是项目提供的： type Config struct &#123; Num int // config number Shards [NShards]int // shard -&gt; gid Groups map[int][]string // gid -&gt; servers[] &#125; Num表示配置的递增id，Shards表示的是某个shard被分配给哪个group，Groups表示的是某一个群组中有哪些server。这个lab里shard总数是固定的值，NShards=10。 实现结果为： # GO111MODULE=off go test Test: Basic leave/join ... ... Passed Test: Historical queries ... ... Passed Test: Move ... ... Passed Test: Concurrent leave/join ... ... Passed Test: Minimal transfers after joins ... ... Passed Test: Minimal transfers after leaves ... ... Passed Test: Multi-group join/leave ... ... Passed Test: Concurrent multi leave/join ... ... Passed Test: Minimal transfers after multijoins ... ... Passed Test: Minimal transfers after multileaves ... ... Passed PASS ok .../MIT6.824/src/shardmaster 8.896s 首先实现第一步是把lab2的框架都拷过来，只是将get和put、append方法改为新的rpc方法。 第二步是设计shardmaster和op结构： type ShardMaster struct &#123; mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg // Your data here. prevOperation map[int64]CommandResponse notifyChanMap map[int]chan CommandResponse configs []Config // indexed by config num &#125; type Op struct &#123; // Your data here. Servers map[int][]string // for Join GIDs []int // for Leave Shard int // for Move GID int // for Move Num int // for Query Type string ClientId int64 CommandId int &#125; type CommandResponse struct &#123; CommandId int Config Config &#125; 为什么op结构是这样的，需要结合这4个方法的参数来解释。实际上lab4A的核心就是实现这四个方法 Jointype JoinArgs struct &#123; Servers map[int][]string // new GID -&gt; servers mappings ClientId int64 CommandId int &#125; Join方法的参数是一个map。Join方法首先要把参数里的server加到group中去 //... newConfig := Config&#123;len(sm.configs), lastConfig.Shards, deepCopy(lastConfig.Groups)&#125; for gid, servers := range groups &#123; // 这里的groups就是参数中的那个map if _, ok := newConfig.Groups[gid]; !ok &#123; newServers := make([]string, len(servers)) copy(newServers, servers) newConfig.Groups[gid] = newServers &#125; &#125; //... 然后进行shard的重分配。这部分leave方法也会用到。 //... s2g := Group2Shards(newConfig) log(fmt.Sprintf(&quot;%+v&quot;, s2g), 1) for &#123; // 这个部分会找出哪个group的shard数量最多，哪个group的shard数量最少 // 然后最多的给一个shard给最少的，循环直到相差数小于等于1 source, target := GetGIDWithMaximumShards(s2g), GetGIDWithMinimumShards(s2g) log(fmt.Sprintf(&quot;source %+v s2g[source] %v target %+v &quot;, source, s2g[source], target), 1) if source != 0 &amp;&amp; len(s2g[source])-len(s2g[target]) &lt;= 1 &#123; break &#125; s2g[target] = append(s2g[target], s2g[source][0]) s2g[source] = s2g[source][1:] &#125; log(fmt.Sprintf(&quot;%+v&quot;, s2g), 1) var newShards [NShards]int for gid, shards := range s2g &#123; for _, shard := range shards &#123; newShards[shard] = gid &#125; &#125; //... // 这个方法的返回值是一个map，key为gid，value为每个group下的shard func Group2Shards(config Config) map[int][]int &#123; s2g := make(map[int][]int) for i, gid := range config.Shards &#123; if _, ok := s2g[gid]; ok &#123; s2g[gid] = append(s2g[gid], i) &#125; else &#123; s2g[gid] = []int&#123;i&#125; &#125; &#125; for gid := range config.Groups &#123; if _, ok := s2g[gid]; !ok &#123; s2g[gid] = []int&#123;&#125; &#125; &#125; return s2g &#125; leaveleave方法的参数是要剔除出去的group：GIDs []int leave方法首选要做的是将group从newConfig.Groups中剔除，然后将它们的shard都存起来（这里变量名为orphanShards） //... newConfig := Config&#123;len(sm.configs), lastConfig.Shards, deepCopy(lastConfig.Groups)&#125; s2g := Group2Shards(newConfig) orphanShards := make([]int, 0) for _, gid := range gids &#123; if _, ok := newConfig.Groups[gid]; ok &#123; delete(newConfig.Groups, gid) &#125; if shards, ok := s2g[gid]; ok &#123; orphanShards = append(orphanShards, shards...) delete(s2g, gid) &#125; &#125; //... 然后同样进行rebalance操作。这里每次都将待分配的shard分配给shard数最少的group //... var newShards [NShards]int if len(newConfig.Groups) != 0 &#123; for _, shard := range orphanShards &#123; target := GetGIDWithMinimumShards(s2g) s2g[target] = append(s2g[target], shard) &#125; for gid, shards := range s2g &#123; for _, shard := range shards &#123; newShards[shard] = gid &#125; &#125; &#125; //... move 与 queryquery方法只需要返回最后一个config。move方法是做一次迁移操作 func (sm *ShardMaster) doMove(shard int, gid int) Err &#123; lastConfig := sm.configs[len(sm.configs)-1] newConfig := Config&#123;len(sm.configs), lastConfig.Shards, deepCopy(lastConfig.Groups)&#125; newConfig.Shards[shard] = gid sm.configs = append(sm.configs, newConfig) return OK &#125; Lab4BLab4B的系统会用到Lab4A的配置管理系统，用shardmaster来负责配置更新与分片分配。然后系统会分出多个raft组来承载所有分片的读写任务。在处理读写的过程中，系统要处理raft组的变更、节点宕机与重启、网络分区等各种情况。同样的，所有操作都要保证线性一致性。 这个lab的实现我能保证前两个test顺利通过，但是在后续test项目中，加入了宕机与重启操作，会有概率无法通过，表现为数据不符合预期或陷入死锁。这里主要分享介绍一下我的思路，后续还需要优化。 实现时，我先参考了作业要求信息的步骤，先实现一个不考虑分片变化的静态系统。这可以直接照搬lab2的代码，但是要注意的是，需要根据提供的算法来决定向哪个分片进行操作： func key2shard(key string) int &#123; shard := 0 if len(key) &gt; 0 &#123; shard = int(key[0]) &#125; shard %= shardmaster.NShards return shard &#125; 每个server有一个gid，表示自己属于哪个raft组。在每次接受到Get、Put、Append请求时，如果发现该server所在的组没有这个分片（根据lab3A的config信息判断），就返回ErrWrongGroup。做完这些之后，可以顺利得通过第一个测试任务TestStaticShards。 但是如果考虑到分片迁移应该怎么办呢？首先，根据课程提示，我们至少要有一个线程来拉取最新的配置，使自身的配置得到更新。其次，我们需要有一个rpc方法，帮助我们从一个raft组传递数据给另一个raft组。当然，这些操作也都必须经过raft层的应用。 有了以上的目标之后，先设计一下相关的数据结构： Op是提交到raft中的结构，可以有为空的字段，但需要包含各种信息。参考了其他人的设计，Op中的CommandType分为Operation、Configuration、UpdateShards这几种。Operation就是客户端读写操作，Configuration是配置更新日志，UpdateShards是分片的数据更新操作。CommandType还可以加一个DeleteShards用来实现lab challenge目标中的gc任务，这里略过。 type Op struct &#123; CommandType CommandType Type string // get put append Key string Value string ClientId int64 CommandId int Config *shardmaster.Config ShardOperationResponse *ShardOperationResponse &#125; server的结构中加了这几个： //... sm *shardmaster.Clerk // 用来发送rpc lastConfig shardmaster.Config // 前一次配置 currentConfig shardmaster.Config // 最新配置 persister *raft.Persister shardDatabase map[int]*Shard // shard database prevOperation map[int64]CommandResponse notifyChanMap map[int]chan CommandResponse updating bool // 这两个与更新shard数据有关 updatedNum int //... Shard结构保存了一个分片的状态和数据： type ShardStatus uint8 const ( Serving ShardStatus = iota ToPull Pulling GCing ) type Shard struct &#123; KV map[string]string Status ShardStatus &#125; func NewShard() *Shard &#123; return &amp;Shard&#123;make(map[string]string), Serving&#125; &#125; 然后在启动方法中加入定时操作的线程： // 定时任务 func (kv *ShardKV) monitor(action func(), timeout time.Duration) &#123; for kv.killed() == false &#123; if _, isLeader := kv.rf.GetState(); isLeader &#123; action() &#125; time.Sleep(timeout) &#125; &#125; // start方法： //... go kv.monitor(kv.configureAction, ConfigureMonitorTimeout) go kv.monitor(kv.migrationAction, MigrationMonitorTimeout) //... 配置更新操作这两个方法，一个是configureAction，用来拉取配置，一个是applyConfiguration，应用配置： func (kv *ShardKV) configureAction() &#123; canPerformNextConfig := true kv.mu.Lock() if kv.updating &#123; canPerformNextConfig = false &#125; currentConfigNum := kv.currentConfig.Num kv.mu.Unlock() if canPerformNextConfig &#123; nextConfig := kv.sm.Query(currentConfigNum + 1) if nextConfig.Num == currentConfigNum+1 &#123; //log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; fetches latest configuration %v when currentConfigNum is %v&quot;, kv.gid, kv.me, nextConfig, currentConfigNum), 1) op := Op&#123;CommandType: Configuration, Config: &amp;nextConfig&#125; kv.executeOp(op) &#125; &#125; &#125; // 在raft中apply成功后调用这个方法 func (kv *ShardKV) applyConfiguration(nextConfig *shardmaster.Config) CommandResponse &#123; if nextConfig.Num == kv.currentConfig.Num+1 &#123; //log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; updates currentConfig from %v to %v&quot;, kv.gid, kv.me, kv.currentConfig, nextConfig), 2) kv.lastConfig = kv.currentConfig kv.currentConfig = *nextConfig return CommandResponse&#123;Err: OK&#125; &#125; //log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; rejects outdated config %v when currentConfig is %v&quot;, kv.gid, kv.me, nextConfig, kv.currentConfig), 1) return CommandResponse&#123;Err: ErrOutDated&#125; &#125; 迁移shard数据的方法也是类似的，一个是migrationAction，另一个是applyUpdateShards。另外还有一个GetShardsData方法为rpc调用。 migrationAction会先通过调用getToPullShardIdMap，根据前一次配置和当前配置去生成更新清单，指向哪些raft组请求哪些raft分片的信息。然后进行分片迁移操作。注意我这里通过设置updating的方式阻塞了读写操作与配置更新操作，这样不符合challenge目标，可以后续通过shard状态来优化。 func (kv *ShardKV) migrationAction() &#123; kv.mu.Lock() if kv.updating &#123; kv.mu.Unlock() return &#125; gid2shardIDs := kv.getToPullShardIdMap() if len(gid2shardIDs) &gt; 0 &#123; log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; migrationAction gid2shardIDs:%+v&quot;, kv.gid, kv.me, gid2shardIDs), 2) &#125; else &#123; kv.mu.Unlock() return &#125; kv.updating = true var wg sync.WaitGroup for gid, shardIDs := range gid2shardIDs &#123; log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; starts a PullTask to get shards %v from group %v when config is %v&quot;, kv.gid, kv.me, shardIDs, gid, kv.currentConfig), 2) wg.Add(1) go func(servers []string, configNum int, shardIDs []int) &#123; defer wg.Done() pullTaskRequest := ShardOperationRequest&#123;configNum, shardIDs&#125; for _, server := range servers &#123; var pullTaskResponse ShardOperationResponse srv := kv.make_end(server) keepTry := true for keepTry &#123; srv.Call(&quot;ShardKV.GetShardsData&quot;, &amp;pullTaskRequest, &amp;pullTaskResponse) if pullTaskResponse.Err == OK &#123; log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125;gets a PullTaskResponse %+v and tries to commit it when currentConfigNum is %v&quot;, kv.gid, kv.me, pullTaskResponse, configNum), 2) op := Op&#123;CommandType: UpdateShards, ShardOperationResponse: &amp;pullTaskResponse&#125; kv.executeOp(op) keepTry = false &#125; else if pullTaskResponse.Err == ErrWrongLeader &#123; keepTry = false &#125; else &#123; log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125;gets a PullTaskResponse %+v currentConfigNum is %v&quot;, kv.gid, kv.me, pullTaskResponse, configNum), 2) time.Sleep(time.Duration(100) * time.Millisecond) &#125; &#125; &#125; &#125;(kv.lastConfig.Groups[gid], kv.currentConfig.Num, shardIDs) &#125; kv.mu.Unlock() wg.Wait() kv.mu.Lock() kv.updating = false kv.updatedNum = kv.currentConfig.Num kv.mu.Unlock() &#125; func (kv *ShardKV) getToPullShardIdMap() map[int][]int &#123; gid2ShardIds := make(map[int][]int) if kv.updatedNum == kv.currentConfig.Num &#123; return gid2ShardIds &#125; for shardId, oldGid := range kv.lastConfig.Shards &#123; if oldGid == 0 &#123; continue // init &#125; if kv.shardDatabase[shardId].Status == ToPull &#123; continue &#125; newGid := kv.currentConfig.Shards[shardId] if newGid == kv.gid &amp;&amp; newGid != oldGid &#123; for _, gId := range kv.lastConfig.Shards &#123; if gId == oldGid &#123; gid2ShardIds[gId] = append(gid2ShardIds[gId], shardId) kv.shardDatabase[shardId].Status = ToPull break &#125; &#125; &#125; &#125; return gid2ShardIds &#125; 在迁移过程中会新开线程做拉取配置操作与向raft提交操作记录 func (kv *ShardKV) GetShardsData(request *ShardOperationRequest, response *ShardOperationResponse) &#123; // only pull shards from leader if _, isLeader := kv.rf.GetState(); !isLeader &#123; response.Err = ErrWrongLeader return &#125; kv.mu.Lock() //defer log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; processes PullTaskRequest %+v with response %+v&quot;, kv.gid, kv.me, *request, *response), 2) defer kv.mu.Unlock() if kv.currentConfig.Num &lt; request.ConfigNum &#123; log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; processes PullTaskRequest %+v ErrOutDated: kv.currentConfig.Num %v, request.ConfigNum %v&quot;, kv.gid, kv.me, *request, kv.currentConfig.Num, request.ConfigNum), 2) response.Err = ErrOutDated return &#125; response.ShardData = make(map[int]map[string]string) for _, shardId := range request.ShardIds &#123; shard := kv.shardDatabase[shardId] response.ShardData[shardId] = shard.deepCopy() &#125; response.PrevOperation = make(map[int64]CommandResponse) for clientID, operation := range kv.prevOperation &#123; response.PrevOperation[clientID] = operation.deepCopy() &#125; response.ConfigNum, response.Err = request.ConfigNum, OK &#125; 最后是应用shard迁移的数据。 func (kv *ShardKV) applyUpdateShards(shardsInfo *ShardOperationResponse) CommandResponse &#123; if shardsInfo.ConfigNum == kv.currentConfig.Num &#123; log(fmt.Sprintf(&quot;&#123;Group %v&#125;&#123;Node %v&#125; accepts shards insertion %v when currentConfig is %v&quot;, kv.gid, kv.me, shardsInfo, kv.currentConfig), 1) for shardId, shardData := range shardsInfo.ShardData &#123; shard := kv.shardDatabase[shardId] for key, value := range shardData &#123; shard.KV[key] = value &#125; shard.Status = Serving &#125; for clientId, commandResponse := range shardsInfo.PrevOperation &#123; kv.prevOperation[clientId] = commandResponse &#125; return CommandResponse&#123;Err: OK&#125; &#125; log(fmt.Sprintf(&quot;&#123;Node %v&#125;&#123;Group %v&#125; rejects outdated shards insertion %v when currentConfig is %v&quot;, kv.me, kv.gid, shardsInfo, kv.currentConfig), 1) return CommandResponse&#123;Err: ErrOutDated&#125; &#125; 这样实现后，可以顺利通过测试2TestJoinLeave。但是在测试3TestSnapshot时遇到了问题。宕机与恢复至少需要实现takeSnapshot与restoreSnapshot这两步，在实现了之后，系统的数据问题与死锁问题频发，可能还需要对这块有更多的调式工作。另外2个challenge也需要进行优化，需要在迁移过程中只阻塞待更新的shard，还需要一个gc方法，具体场景为，在迁移走数据shard数据后，raft组会清空已不需要的数据。","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"MIT 6.824 Lab3","slug":"2022/6.824Lab3","date":"2022-10-20T15:59:59.000Z","updated":"2023-04-09T15:51:46.136Z","comments":true,"path":"2022/10/20/2022/6.824Lab3/","link":"","permalink":"https://juniousy.github.io/2022/10/20/2022/6.824Lab3/","excerpt":"","text":"Lab3的目标是利用Raft的机制，实现一个线性一致（Linearizability）的key-value存储结构。 Lab3A概述通过测试 Test: one client (3A) ... labgob warning: Decoding into a non-default variable/field Err may not work ... Passed -- 15.2 5 7307 289 Test: many clients (3A) ... ... Passed -- 15.9 5 12944 1483 Test: unreliable net, many clients (3A) ... ... Passed -- 16.1 5 3911 1029 Test: concurrent append to same key, unreliable (3A) ... ... Passed -- 1.2 3 163 52 Test: progress in majority (3A) ... ... Passed -- 1.2 5 167 2 Test: no progress in minority (3A) ... ... Passed -- 1.1 5 191 3 Test: completion after heal (3A) ... ... Passed -- 1.1 5 96 3 Test: partitions, one client (3A) ... ... Passed -- 22.4 5 15465 175 Test: partitions, many clients (3A) ... ... Passed -- 23.2 5 23550 1158 Test: restarts, one client (3A) ... ... Passed -- 21.3 5 21560 289 Test: restarts, many clients (3A) ... ... Passed -- 23.3 5 105143 1445 Test: unreliable net, restarts, many clients (3A) ... ... Passed -- 23.6 5 5069 1145 Test: restarts, partitions, many clients (3A) ... ... Passed -- 29.1 5 78246 825 Test: unreliable net, restarts, partitions, many clients (3A) ... ... Passed -- 32.1 5 4443 507 Test: unreliable net, restarts, partitions, many clients, linearizability checks (3A) ... ... Passed -- 28.8 7 9497 887 PASS 一开始看到Lab3A的时候完全不知道从何入手，仔细看了下的话，其实kv server分为三个部分：Client, Server, Raft。其中Raft是Server的底层，是分布式Server做到线性一致的基础。这其中的通信关系是：Client &lt;=&gt; Server, Server &lt;=&gt; Raft, Raft &lt;=&gt; Raft。有两个重要的点，一个是Server和Server之间是不通信的，必须通过Raft通信，第二个是Server和Raft是一对一的关系，只有Raft leader对应的Server才有操作的资格。另外有一个细节是，在这个lab里，每个Get请求也必须通过raft协议才能完成，这样做的目的是保证线性一致。 Client首先从Cliet处着手。保证消息不重复消费最重要的一部分是Client会有自己的clientId，每次指令也有一个commandId。Client的Get和Put方法就是循环调用server的接口，如果不成功就换下一个server，直到server返回成功。 for &#123; var getArgs GetArgs = GetArgs&#123; Key: key, ClientId: ck.clientId, CommandId: ck.commandId, &#125; var getReply GetReply for &#123; ok := ck.servers[ck.leaderId].Call(&quot;KVServer.Get&quot;, &amp;getArgs, &amp;getReply) if !ok || getReply.Err == ErrWrongLeader || getReply.Err == ErrTimeout &#123; ck.leaderId = (ck.leaderId + 1) % (len(ck.servers)) continue &#125; DPrintf(&quot;###client %v Get ok %#v, commandId:&#39;%v&#39; \\n&quot;, ck.clientId, getReply, ck.commandId) ck.commandId++ return getReply.Value &#125; &#125; ServerServer端会比较复杂。Server和Client通信是用的RPC，那么Server和Raft是怎么通信的呢？答案是Server向Raft发起请求是通过Raft的Start()入口方法，然后通过Raft的applyCh这个channel来获取数据。因此在Server新建的时候，需要新开一个线程用来获取applyCh的数据。 Start方法传入的数据结构会存到Raft的日志中： type Op struct &#123; Type string // get put append Key string Value string ClientId int64 CommandId int &#125; Server在收到Get、Put、Append的RPC请求时，要做这几件事： 构造Op参数 根据ClientId和CommandId判断是不是重复请求 调用Raft的Start方法 收到Start方法结果，如果返回值中isLeader是false，就返回ErrWrongLeader给Client 生成一个channel，在applyCh数据返回后给 apply message 线程后，线程会把数据传回到这个channel。接收成功就可以返回给client结果。 这个channel需要做超时处理： DPrintf(&quot;leader %v 开始等待PutAppend结果: index %v, isLeader %v, args %+v&quot;, kv.me, index, isLeader, args) ch := kv.makeNotifyChan(index) kv.mu.Unlock() select &#123; case &lt;-time.After(ExecuteTimeout): DPrintf(&quot;leader %v PutAppend 超时: index %v, isLeader %v, args %+v&quot;, kv.me, index, isLeader, args) reply.Err = ErrTimeout case &lt;-ch: reply.Err = OK DPrintf(&quot;leader %v PutAppend结果得到: index %v, isLeader %v, args %+v, reply %+v&quot;, kv.me, index, isLeader, args, reply) &#125; go func() &#123; kv.closeNotifyChan(index) &#125;() apply message 线程就是循环读取applyCh传来的数据，如果是写入操作，就把操作写入server的数据库中。在本lab里，这个操作是写入到一个map里。这里有一个重要的点，是判断applyCh传来的数据是不是过时的数据。因此，我的做法是在server中维护一个结构prevOperation map[int64]CommandResponse，记录每个Client上一次返回结果。CommandResponse中有CommandId信息。如果applyCh传来的数据中，它的CommandId小于等于前一次返回结果的CommandId，就直接抛弃这条消息。最后server把数据传回给rpc请求： ch := kv.getNotifyChan(message.CommandIndex) if ch != nil &#123; ch &lt;- response &#125; 这里有个细节是如果拿不到channel了，就说明rpc方法已经超时返回了，删除了这个channel。这里的处理是试了很多次之后定下来的，能通过测试，但可能还是有点疑问。因为如果这里不传消息，其实是相当于丢了一次已经记录到Raft中的消息，而如果不传，那这里因为rpc已经返回了，所以channel会永远在等待发送消息中。参考了一下其他人的实现，有的人会说这里不会阻塞，可能是实现的一些细节不同吧。 整个LAB3A做下来的感受其实是挺折磨的，最大的问题是很容易遇到死锁问题，需要写大量的log，尤其是在加锁解锁和channel通信处。下面举几个例子： 死锁一是在上面说的阻塞的情况时发生： // putappend DPrintf(&quot;putAppend 0&quot;) kv.mu.Lock() DPrintf(&quot;putAppend 1&quot;) // dosomething kv.mu.Unlock() // apply线程 kv.mu.Lock() ... DPrintf(&quot;apply 1&quot;) ch &lt;- response DPrintf(&quot;apply 2&quot;) kv.mu.Unlock() DPrintf(&quot;apply 3&quot;) // 输出结果 apply 1 putAppend 1 putAppend 0 由于apply处有锁，而apply阻塞等待消息，因此rpc方法处的锁无法拿到。这里的错误点在于发送消息时应该是没有锁的。要么提前解锁，要么新开一个线程发送消息。 死锁二，apply必须在是leader情况下时才能发送管道消息，不然就永远在等待。为什么呢？因为只有在leader情况下才会新建chanel。当然上面判断channel是不是为空也是一种解决方法，但更正确的做法应该是只在是leader而且term相符情况下才发送消息： currentTerm, isLeader := kv.rf.GetState() if isLeader &amp;&amp; message.CommandTerm == currentTerm &#123; // send &#125; 死锁三是在raft中发生的，这算是之前lab2中一个没有暴露出来的bug。之前在raft发送applyCh消息时，是持有raft的锁的。当apply一次性提交很多个数据时，会一直占用rf.mu。但是在Server处，会调用rf.Start()或者rf.GetState()，这两个都要求锁。于是，Server端等待rf的锁，无法处理applyCh的下一条消息，而raft持有锁，等待向applyCh中发送消息，于是引发了死锁。解决方法和上面一样，在消息发送时，要么解锁，要么新开线程： // 问题代码： // server层 currentTerm, isLeader := kv.rf.GetState() // raft层 // 加锁状态 一次有多个msg发送 rf.applyCh &lt;- msg // 需要改成： rf.mu.Unlock() rf.applyCh &lt;- msg rf.mu.Lock() Lab3BTest: InstallSnapshot RPC (3B) ... ... Passed -- 7.5 3 4540 63 Test: snapshot size is reasonable (3B) ... ... Passed -- 41.2 3 10828 800 Test: restarts, snapshots, one client (3B) ... ... Passed -- 20.8 5 17973 289 Test: restarts, snapshots, many clients (3B) ... ... Passed -- 25.3 5 129392 5900 Test: unreliable net, snapshots, many clients (3B) ... ... Passed -- 16.0 5 3476 848 Test: unreliable net, restarts, snapshots, many clients (3B) ... ... Passed -- 23.5 5 4381 730 Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ... ... Passed -- 32.0 5 3448 343 Lab3B要做的事情很简单，就是将日志压缩为snapshop。但实际我自己做下来比lab3A要繁琐得多，也尝试了很久。而且虽然通过了所有的测试，但最后三个测试有小概率失败，也很难定位到问题。 具体来说，Lab3B的流程是，server在发现日志大小超过某个临界值之后，将自己的数据序列化存储为快照，然后传给raft。raft会删除旧的日志，保留必要的信息。 Server层server比较简单，但也会有坑。首先在apply线程中，在拿到raft返回数据后，判断要不要生成快照： func (kv *KVServer) needSnapshot() bool &#123; if kv.maxraftstate == -1 &#123; return false &#125; return kv.persister.RaftStateSize() &gt; kv.maxraftstate &#125; 如果需要，就要讲自己的数据map、lastAppliedIndex表和prevOperation表序列化生成快照。后面两个在恢复server的时候是非常有必要的，防止多次提交。在序列化的最后，是调取一个新的Raft方法传递当前的日志index（很重要）和快照数据kv.rf.TakeSnapshot(index, buffer.Bytes())。 反过来也需要一个反序列化的读取方法，将这些数据应用到Server层中。触发的时间点是Raft通过applyCh将操作成功的消息传递给Server层时。一个细节是启动时候也要进行这一步。 Raft层首先，Raft层接受Server命令的方法会进行删除日志、记录快照最后包含的Index。Raft层会新增两个字段lastIncludedIndex、lastIncludedTerm。然后leader将快照数据持久化。 然后，Raft层会多一个RPC请求，这是leader向follower发送snapshot命令的请求： func (rf *Raft) InstallSnapshot(args *InstallSnapshotArgs, reply *InstallSnapshotReply) &#123; // ... 省略判断校验逻辑 logs := make([]Log, 0) startIndex := rf.newIndex(args.LastIncludedIndex + 1) if startIndex &lt;= len(rf.logEntries) &#123; logs = append(logs, rf.logEntries[startIndex:]...) &#125; rf.logEntries = logs rf.lastIncludedIndex = args.LastIncludedIndex rf.lastIncludedTerm = args.LastIncludedTerm rf.lastApplied = max(rf.lastIncludedIndex, rf.lastApplied) rf.commitIndex = max(rf.lastIncludedIndex, rf.commitIndex) // ... 省略持久化数据 // ... 省略发送消息给Server层 &#125; 这个核心方法是删除日志，然后将相关index修改。 那么，什么时候leader向server发送这一消息呢？我的处理是这样的，每次leader持久化之后就对所有follower进行一次发送。然后，在发送AppendEntries处，如果nextIndex的值已经和日志不匹配了，就说明也要发送。这种情况是follower脱离集群之后再回来会发生的事： prevLogIndex := rf.nextIndex[id] - 1 if rf.newIndex(prevLogIndex) &gt;= len(rf.logEntries) || rf.newIndex(prevLogIndex) &lt; -1 &#123; //fmt.Printf(&quot;#### rf.me %v rf.nextIndex[%v] = %v, rf.lastIncludedIndex %v\\n&quot;, rf.me, id, rf.nextIndex[id], rf.lastIncludedIndex) rf.mu.Unlock() rf.sendSnapshot(id) return &#125; 这里有一个非常头疼的地方是，下标index怎么处理。日志list是新建的，原来的下标是对应不上的。有两种思路，一种是，在新建日志时，重算所有的下标并更新；另一种思路是，所有的下标保持原样，在使用到日志list上时做处理。一开始，我在尝试了一下第二种思路后，选择放弃了，因为要改的地方很多，以为前一种思路更简单一点，因为只要改一处地方。但实际上试下来非常头疼，debug了很久，因为很容易出现旧下标和新下标混用的情况。具体遇到的问题如：1. Server层在get方法时，apply线程将信息发给了别的rpc接口。因为获取channel的参数是Start方法的返回值中的index，一旦index变化，就会发错数据。2. 在解决了问题一之后，Raft层的数据有概率出现错配的情况，也很难调试出原因。然后我重看了一下论文，再参考了一下别人的实现，思考了一下之后我觉得只有第二种才是正常的实现，因为各种index（如applied index）必须是递增的，没有理由去改动它们。所以，必须在读取日志list时做改动： func (rf *Raft) newIndex(index int) int &#123; return index - rf.lastIncludedIndex - 1 &#125; func (rf *Raft) trueIndex(index int) int &#123; return index + rf.lastIncludedIndex + 1 &#125; 这里面头疼的点是，有些不是直接和list下标有关的方法内部参数，也需要改动，举一个例子，选举时有一个参数lastLogIndex，就需要算成实际的下标来使用，否则出现的情况是，一个没有数据的新follower会因为term较高而被选举为leader。 解决为下标问题，Lab3B就没有什么大的问题了。总的来说，Lab3需要谨慎规避死锁问题，需要在多线程分布式环境下打充足的日志来调试问题。功能点上可能不是很多，但是很能锻炼工程实现能力和复杂问题处理能力","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"非递归中序遍历二叉树","slug":"2022/非递归中序遍历二叉树","date":"2022-09-19T17:37:00.000Z","updated":"2022-09-20T16:36:14.326Z","comments":true,"path":"2022/09/20/2022/非递归中序遍历二叉树/","link":"","permalink":"https://juniousy.github.io/2022/09/20/2022/%E9%9D%9E%E9%80%92%E5%BD%92%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"举例相关leetcode题： 94. 二叉树的中序遍历递归我就不说了 迭代（重点是使用栈）：class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); Deque&lt;TreeNode&gt; stk = new LinkedList&lt;TreeNode&gt;(); while (root != null || !stk.isEmpty()) &#123; while (root != null) &#123; stk.push(root); root = root.left; &#125; root = stk.pop(); res.add(root.val); root = root.right; &#125; return res; &#125; &#125; 染色法参考资料看到的，也是用到了栈，不过会进行“染色”，规则为： 使用颜色标记节点的状态，新节点为白色，已访问的节点为灰色。 如果遇到的节点为白色，则将其标记为灰色，然后将其右子节点、自身、左子节点依次入栈。 如果遇到的节点为灰色，则将节点的值输出。 class Solution: def inorderTraversal(self, root: TreeNode) -&gt; List[int]: WHITE, GRAY = 0, 1 res = [] stack = [(WHITE, root)] while stack: color, node = stack.pop() if node is None: continue if color == WHITE: stack.append((WHITE, node.right)) stack.append((GRAY, node)) stack.append((WHITE, node.left)) else: res.append(node.val) return res 如要实现前序、后序遍历，只需要调整左右子节点的入栈顺序即可。 Morris 中序遍历Morris 遍历算法是另一种遍历二叉树的方法，它能将非递归的中序遍历空间复杂度降为 O(1)。 Morris 遍历算法整体步骤如下（假设当前遍历到的节点为 xx）： 如果 xx 无左孩子，先将 xx 的值加入答案数组，再访问 xx 的右孩子，即 x = x.\\textit{right}x=x.right。 如果 xx 有左孩子，则找到 xx 左子树上最右的节点（即左子树中序遍历的最后一个节点，xx 在中序遍历中的前驱节点），我们记为predecessor。根据 predecessor 的右孩子是否为空，进行如下操作。 如果 predecessor 的右孩子为空，则将其右孩子指向 xx，然后访问 xx 的左孩子，即 x=x.left。 如果 predecessor 的右孩子不为空，则此时其右孩子指向 xx，说明我们已经遍历完 xx 的左子树，我们将 predecessor 的右孩子置空，将 xx 的值加入答案数组，然后访问 xx 的右孩子，即 x=x.right。 重复上述操作，直至访问完整棵树。 class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); TreeNode predecessor = null; while (root != null) &#123; if (root.left != null) &#123; // predecessor 节点就是当前 root 节点向左走一步，然后一直向右走至无法走为止 predecessor = root.left; while (predecessor.right != null &amp;&amp; predecessor.right != root) &#123; predecessor = predecessor.right; &#125; // 让 predecessor 的右指针指向 root，继续遍历左子树 if (predecessor.right == null) &#123; predecessor.right = root; root = root.left; &#125; // 说明左子树已经访问完了，我们需要断开链接 else &#123; res.add(root.val); predecessor.right = null; root = root.right; &#125; &#125; // 如果没有左孩子，则直接访问右孩子 else &#123; res.add(root.val); root = root.right; &#125; &#125; return res; &#125; &#125; 特点是空间复杂度为O(1)、会改变原来树的结构 98. 验证二叉搜索树（给你一个二叉树的根节点 root ，判断其是否是一个有效的二叉搜索树）这是一道相关的题目 class Solution &#123; public boolean isValidBST(TreeNode root) &#123; Deque&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); long preVal = Long.MIN_VALUE; TreeNode node = root; while(!stack.isEmpty() || node != null) &#123; while(node != null) &#123; stack.push(node); node = node.left; &#125; node = stack.pop(); if(node.val &lt;= preVal) &#123; return false; &#125; preVal = node.val; node = node.right; &#125; return true; &#125; &#125; 另外看到这题有一个很优雅的写法： class Solution &#123; public boolean isValidBST(TreeNode root) &#123; return isValid(root, Long.MIN_VALUE, Long.MAX_VALUE); &#125; private boolean isValid(TreeNode node, long min, long max) &#123; if (node == null) &#123; return true; &#125; if (node.val &lt;= min || node.val &gt;= max)&#123; return false; &#125; return isValid(node.left, min, node.val) &amp;&amp; isValid(node.right, node.val, max); &#125; &#125;","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://juniousy.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"MIT 6.824 Lab2C","slug":"2022/6.824Lab2C","date":"2022-08-29T15:30:30.000Z","updated":"2023-02-23T14:22:52.491Z","comments":true,"path":"2022/08/29/2022/6.824Lab2C/","link":"","permalink":"https://juniousy.github.io/2022/08/29/2022/6.824Lab2C/","excerpt":"","text":"2C的持久化本身比较简单，就是把currentTerm、voteFor、logEntries存和取就行。需要注意的点的存数据的时间点应该在这三个值有变化的时候，准确来说是这两个地方：一个是在变为follower和candidate的时候，另一个是在commit data之后。另外尤其强调是在“之后”做持久化，否则会把未提交的数据保留下来。 但是实际跑测试的时候有一个测试怎么都通不过，试了很多次，然后试了很多日志终于发现不是持久化的问题。 GO111MODULE=off go test -run 2C Test (2C): basic persistence ... ... Passed -- 5.6 3 100 24433 6 Test (2C): more persistence ... ... Passed -- 18.0 5 988 204042 16 Test (2C): partitioned leader and one follower crash, leader restarts ... ... Passed -- 2.4 3 34 8699 4 Test (2C): Figure 8 ... ... Passed -- 31.4 5 560 106887 8 Test (2C): unreliable agreement ... ... Passed -- 5.6 5 216 76193 246 Test (2C): Figure 8 (unreliable) ... --- FAIL: TestFigure8Unreliable2C (45.35s) config.go:478: one(8275) failed to reach agreement Test (2C): churn ... ... Passed -- 16.3 5 608 284683 268 Test (2C): unreliable churn ... ... Passed -- 16.2 5 932 502715 249 FAIL exit status 1 TestFigure8Unreliable2C是在测什么呢？对于普通的Figure 8 test，测试方法会不断地断线、宕机、重连服务，而加上Unreliable这个设定会让rpc请求随机失败或者延迟。通过Figure 8 test的核心点在于不要存未commit的数据（具体图见论文Figure 8）。 而TestFigure8Unreliable2C的失败结果初看非常奇怪。在跑了无数次TestFigure8Unreliable2C后发现错误的结果大概率是这样的：5个server，4个commit结果完成一致，而另一个则完全没有commit或者只commit了前面很少的一部分，甚至可能有全量的log data但是一条都没有提交。 以这次测试结果为例： Test (2C): Figure 8 (unreliable) ... *** server 1 成为 leader, currentTerm 1 *** leader 1 更新commitIndex 1, 原commitIndex 0 *** server 4 成为 leader, currentTerm 3 *** *** server 3 成为 leader, currentTerm 4 *** *** server 3 成为 leader, currentTerm 7 *** leader 3 更新commitIndex 17, 原commitIndex 0 leader 3 更新commitIndex 20, 原commitIndex 17 *** server 0 成为 leader, currentTerm 10 *** *** server 2 成为 leader, currentTerm 11 *** *** server 1 成为 leader, currentTerm 36 *** *** server 0 成为 leader, currentTerm 49 *** *** server 1 成为 leader, currentTerm 51 *** leader 1 更新commitIndex 184, 原commitIndex 20 *** server 4 成为 leader, currentTerm 55 *** *** server 4 成为 leader, currentTerm 64 *** *** server 3 成为 leader, currentTerm 77 *** *** server 3 成为 leader, currentTerm 93 *** leader 3 更新commitIndex 327, 原commitIndex 184 leader 3 更新commitIndex 328, 原commitIndex 327 leader 3 更新commitIndex 329, 原commitIndex 328 leader 3 更新commitIndex 330, 原commitIndex 329 最后结果是server1-4的结果都是对的，只有server0的commit index停留在20。 为什么呢？再打日志发现，TestFigure8Unreliable2C最后会让服务全部连线上（但是rpc请求还是可能失败），此时leader不断地向原来被分割开的服务发送同样的请求，而返回也是同样的coflict term 和 conflict index。这两个值是server告诉leader，你传的数据不对，要从这个地方重新传。 看下原来leader处的处理： // 有冲突的情况 if reply.ConflictTerm != -1 &#123; for i, v := range rf.logEntries &#123; if v.Term == reply.ConflictTerm &#123; rf.nextIndex[id] = i break &#125; &#125; &#125; else &#123; rf.nextIndex[id] = reply.ConflictIndex &#125; 从表现上看，这里一直在根据coflict term决定下一次传的index id会陷入调用上的死循环。所以在此处，更应该注意follower自己发的conflict index。于是修改后加了一条逻辑： if reply.ConflictTerm != -1 &#123; if rf.logEntries[reply.ConflictIndex].Term == reply.ConflictTerm &#123; for i, v := range rf.logEntries &#123; if v.Term == reply.ConflictTerm &#123; rf.nextIndex[id] = i break &#125; &#125; &#125; else &#123; rf.nextIndex[id] = reply.ConflictIndex &#125; &#125; else &#123; rf.nextIndex[id] = reply.ConflictIndex &#125; 这样做就解决了问题，最后顺利通过全部测试。 GO111MODULE=off go test -run 2C Test (2C): basic persistence ... ... Passed -- 4.2 3 114 29456 6 Test (2C): more persistence ... ... Passed -- 22.7 5 2125 472533 19 Test (2C): partitioned leader and one follower crash, leader restarts ... ... Passed -- 3.9 3 102 28521 4 Test (2C): Figure 8 ... ... Passed -- 35.1 5 676 119626 7 Test (2C): unreliable agreement ... ... Passed -- 3.0 5 216 75708 246 Test (2C): Figure 8 (unreliable) ... ... Passed -- 37.3 5 4612 7879256 172 Test (2C): churn ... ... Passed -- 16.3 5 1240 1145184 697 Test (2C): unreliable churn ... ... Passed -- 16.1 5 1220 879587 526 PASS ok .../src/raft 138.617s 最后，我自己总结出的本项目的打日志方法，最主要的是这么几个地方：选取出leader处、leader提交数据处。其他地方也需要酌情打日志，但是很可能信息太多，淹没了有用的信息。 Lab的测试期望在4分钟内跑完，最后2A8秒，2B约38秒，2C约138秒，至少在我的电脑上达到标准了。","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"MIT 6.824 Lab2B","slug":"2022/6.824Lab2B","date":"2022-08-08T17:01:45.000Z","updated":"2022-08-30T15:39:57.253Z","comments":true,"path":"2022/08/09/2022/6.824Lab2B/","link":"","permalink":"https://juniousy.github.io/2022/08/09/2022/6.824Lab2B/","excerpt":"","text":"这部分是Raft的核心，先上通过记录。 GO111MODULE=off go test -run 2B Test (2B): basic agreement ... ... Passed -- 1.1 3 16 4688 3 Test (2B): RPC byte count ... ... Passed -- 2.8 3 48 114908 11 Test (2B): agreement despite follower disconnection ... ... Passed -- 6.7 3 124 33767 8 Test (2B): no agreement if too many followers disconnect ... ... Passed -- 4.2 5 212 42984 3 Test (2B): concurrent Start()s ... ... Passed -- 0.8 3 10 2890 6 Test (2B): rejoin of partitioned leader ... ... Passed -- 6.9 3 172 43180 4 Test (2B): leader backs up quickly over incorrect follower logs ... ... Passed -- 28.6 5 2308 1695419 103 Test (2B): RPC counts aren&#39;t too high ... ... Passed -- 5.8 3 100 29488 12 PASS ok ../MIT6.824/src/raft 56.896s 一开始进行任务拆分。实际上除了任务1和2，对其他任务是一起推进实现的，因为很多逻辑都是上下游的关系。 实现Start方法，实现leader自己更新记录 server通过applyCh返回结果 leader向follower发送AppendEntries follower接收处理AppendEntries leader得到follower返回结果（或返回失败）时的处理 确保选举成功的candidate包含所有已提交的记录（实现election restriction） 实现细节1 Start方法和Apply方法Start方法就是业务层向Raft层提交任务的唯一入口。Leader在这个方法里添加log rf.logEntries = append(rf.logEntries, Log&#123;rf.currentTerm, command&#125;)。有个小细节是，可以在此处设置 rf.matchIndex[rf.me] = index，index为最后一个元素的index。这样做的好处是在后续计算能否commit记录时好处理一些。 Apply方法就是Raft层向业务层返回结果的唯一出口。 type ApplyMsg struct &#123; CommandValid bool Command interface&#123;&#125; CommandIndex int &#125; type Raft struct &#123; //... applyCh chan ApplyMsg //... &#125; func (rf *Raft) startApplyLogs() &#123; for rf.lastApplied &lt; rf.commitIndex &#123; msg := ApplyMsg&#123;&#125; rf.lastApplied++ msg.CommandIndex = rf.lastApplied msg.Command = rf.logEntries[rf.lastApplied].Command msg.CommandValid = true //DPrintf(&quot;server %d: 开始应用log, commitIndex: %d, lastApplied: %d, msg %v &quot;, rf.me, rf.commitIndex, rf.lastApplied, msg) rf.applyCh &lt;- msg &#125; &#125; 2 AppendEntriesserver在接受AppendEntries时，至少要有以下校验逻辑： 如果args.PrevLogIndex比自身log最后一个元素的index还要大，就返回ConflictIndex=len(rf.logEntries) 如果args.PrevLogTerm和自身的PrevLogTerm不同，返回ConflictTerm = thisPrevLogTerm、ConflictIndex为该Term下的第一个index 普通server的commitIndex更新逻辑： if args.LeaderCommit &gt; rf.commitIndex &#123; rf.commitIndex = int(math.Min(float64(args.LeaderCommit), float64(len(rf.logEntries)-1))) &#125; leader在发送AppendEntries后，要对server返回结果进行处理，至少要对以下异常情况进行校验： 没有收到RPC结果，则不进行后续逻辑 term小于返回值的term，则变为leader ConflictTerm或ConflictIndex有值时，重设nextIndex[id] //id为server的id，返回，等待下一次发送时从冲突处发送记录 最后实现Figure 2中的这条：如果存在 N，N &gt; commitIndex，大部分matchIndex[i] &gt;= N，log[N].term == currentTerm，那么，就设置commitIndex = N。Leader的commitIndex在此处更新。 3 election restriction按照up-to-date的定义，follower在vote时要判断该布尔值：upToDate := args.LastLogTerm &lt; lastLogTerm || (args.LastLogTerm == lastLogTerm &amp;&amp; args.LastLogIndex &lt; lastLogIndex)如果为true，说明follower不该向这个candidate投出这票，因为follower相对这个candidate更有资格成为leader。这个判断很关键，后续问题点3这个例子就是不判断时会出现的情况。 实现中遇到的问题问题点1 - TestBasicAgree2BTestBasicAgree2B 偶尔会无法通过。 多试几次后发现，原因是 leader还未发送AppendEntries就有新的candidate出现并成功选举为leader。而前leader并没有丢弃自身的log，导致不一致。 2022/08/06 21:46:22 @@@ Leader 2: got a new Start task, command: 100 2022/08/06 21:46:22 server 1 成为 candidate, currentTerm 2 2022/08/06 21:46:22 === Candidate 1 开始发送 RequestVote, currentTerm 2 === 2022/08/06 21:46:22 server 2 (term 1 voteFor 2) 收到 candidiate 1 (term 2 candidateId 1) 的RequestVote 2022/08/06 21:46:22 server 2 成为 follower，currentTerm 1 ==&gt; 2, leader id 2 ==&gt; -1 2022/08/06 21:46:22 server 0 (term 1 voteFor 2) 收到 candidiate 1 (term 2 candidateId 1) 的RequestVote 2022/08/06 21:46:22 server 0 投票给 server 1 2022/08/06 21:46:22 *** server 1 成为 leader, currentTerm 2 *** 2022/08/06 21:46:22 === server 0 处理 Leader 1 的 AppendEntries 成功，当前logEntries [] === 2022/08/06 21:46:22 === server 2 处理 Leader 1 的 AppendEntries 成功，当前logEntries [&#123;1 100&#125;] === 一方面 server 要放弃之后的内容 ... endIndex := args.PrevLogIndex + len(args.Entries) + 1 if endIndex &lt; len(rf.logEntries) &#123; rf.logEntries = rf.logEntries[:endIndex] &#125; ... 另一方面，根据election restriction机制，一个candidate必须包含所有已提交的entries。具体实现方式是：投票这如果发现自己的log比candidate更新，则不投票。更（第四声）新（up-to-date）的含义是，如果末尾entries的term更大则更新，如果term一样，则log长度长的算更新。具体实现见上文。 在本例中，尽管实现了这一机制，但是3个server中，哪怕之前收到log的前leader2没有投票，server1 有自己一票和server0 一票，依然能当选。 再仔细想一下，这样的结果其实是正确的，因为该entry确实没有被提交。但TestBasicAgree2B是要求每次请求都成功写入的（毕竟确实没有异常出现）。究其原因，最大的问题还是出在选举时，server2明明已经选上leader了，结果server1没有收到心跳。 再检查一下代码： func (rf *Raft) candidateMainFlow() &#123; ... rf.startRequestVote() // line a 此处会成为leader time.Sleep(time.Duration(electionTimeout) * time.Millisecond) // line b 此处会等待 rf.mu.Lock() if rf.state == CANDIDATE &amp;&amp; !rf.heartBeat &#123; rf.convertToCandidate() &#125; rf.mu.Unlock() &#125; 成为leader后没有立刻发送心跳，反而进行了一次等待。所以应该在上述代码中的line a 和 line b中间发送一次空的AppendEntries rf.mu.Lock() isLeader := rf.state == LEADER rf.mu.Unlock() if isLeader &#123; rf.startAppendEntries() return &#125; 但是这样还是会出错。检查发现测试程序一但发现有leader产生之后就会写命令。所以最后引入了firstHeartBeat这个布尔值，在第一次心跳发送前，不允许接受命令，也就是Start方法处的返回结果isLeader是false。这实际是个非标准的做法，不过确实有用。 单改这个地方还不够，另一个测试如果不能在发现有leader后立刻能够插入数据也会报错，所以GetState处的结果也不能立刻返回是不是leader，最后加了一下比较tricky的解决： func (rf *Raft) GetState() (int, bool) &#123; var term int var isleader bool // Your code here (2A). rf.mu.Lock() term = rf.currentTerm isleader = rf.state == LEADER for isleader &amp;&amp; !rf.firstHeartBeat &#123; rf.mu.Unlock() time.Sleep(time.Duration(5) * time.Millisecond) rf.mu.Lock() isleader = rf.state == LEADER &#125; rf.mu.Unlock() return term, isleader &#125; 问题点2 - TestFailNoAgree2BTestFailNoAgree2B 会在5个server中让3个server（不包含原leader）离线，然后进行操作。具体出现问题的测试代码为： // 3 of 5 followers disconnect leader := cfg.checkOneLeader() cfg.disconnect((leader + 1) % servers) cfg.disconnect((leader + 2) % servers) cfg.disconnect((leader + 3) % servers) index, _, ok := cfg.rafts[leader].Start(20) if ok != true &#123; t.Fatalf(&quot;leader rejected Start()&quot;) &#125; if index != 2 &#123; t.Fatalf(&quot;expected index 2, got %v&quot;, index) 这时候由于半数宕机，理论上命令不会被写入。不过在我一开始的实现中，没有离线的两个server依然会写入，这样就有问题。 检查代码发现是applyIndex和commitIndex的赋值存在bug，解决后通过。 问题点3 - TestRejoin2BTestRejoin2B 是这么操作的： leader1 写入 101 leader1 离线 leader1 写入 102 103 104 leader2 （新的leader） 写入 103 leader2 离线 leader1 上线 leader1 写入104 leader2 上线 写入 105 遇到的问题是leader1重新上线后无法选举出新的leader。如下面的例子，server2 和 server 0互相都不投给对方vote connect(2) 2022/08/07 00:41:56 @@@ Leader 2: got a new Start task, command: 104 2022/08/07 00:41:56 Leader 2 发送给 server 1 AppendEntries，args: &#123;Term:1 LeaderId:2 PrevLogIndex:1 PrevLogTerm:1 Entries:[&#123;Term:1 Command:102&#125; &#123;Term:1 Command:103&#125; &#123;Term:1 Command:104&#125; &#123;Term:1 Command:104&#125;] LeaderCommit:1&#125; 2022/08/07 00:41:56 Leader 2 发送给 server 0 AppendEntries，args: &#123;Term:1 LeaderId:2 PrevLogIndex:1 PrevLogTerm:1 Entries:[&#123;Term:1 Command:102&#125; &#123;Term:1 Command:103&#125; &#123;Term:1 Command:104&#125; &#123;Term:1 Command:104&#125;] LeaderCommit:1&#125; 2022/08/07 00:41:56 server 2 成为 follower，currentTerm 1 ==&gt; 2, leader id 2 ==&gt; -1 2022/08/07 00:41:57 server 0 成为 candidate, currentTerm 3 2022/08/07 00:41:57 === Candidate 0 开始发送 RequestVote, currentTerm 3 === 2022/08/07 00:41:57 server 2 (term 2 voteFor -1) 收到 candidiate 0 (term 3 candidateId 0) 的RequestVote 2022/08/07 00:41:57 server 2 成为 candidate, currentTerm 4 2022/08/07 00:41:57 === Candidate 2 开始发送 RequestVote, currentTerm 4 === 2022/08/07 00:41:57 server 0 (term 3 voteFor 0) 收到 candidiate 2 (term 4 candidateId 2) 的RequestVote 2022/08/07 00:41:57 server 0 成为 follower，currentTerm 3 ==&gt; 4, leader id 0 ==&gt; -1 2022/08/07 00:41:57 server 2 成为 candidate, currentTerm 5 这个问题是由于vote时候判断up-to-date的逻辑有问题导致的，上面已经讲过。修改后这个问题解决。","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"MIT 6.824 Lab2A","slug":"2022/6.824Lab2A","date":"2022-07-17T17:18:50.000Z","updated":"2022-08-29T16:01:17.684Z","comments":true,"path":"2022/07/18/2022/6.824Lab2A/","link":"","permalink":"https://juniousy.github.io/2022/07/18/2022/6.824Lab2A/","excerpt":"","text":"光看记录的一些笔记和要点确实有点一头雾水，不知道从何下手，好在Lab分阶段进行，先从最基本的地方开始。2A的测试，一个是正常情况下选举出leader，另一个是模拟leader宕机后重新选举。完成记录如下： GO111MODULE=off go test -v -run 2A 2022/06/26 14:41:54 server 0 成为 follower，currentTerm 0 ==&gt; 0, leader id 0 ==&gt; -1 2022/06/26 14:41:54 server 1 成为 follower，currentTerm 0 ==&gt; 0, leader id 0 ==&gt; -1 2022/06/26 14:41:54 server 2 成为 follower，currentTerm 0 ==&gt; 0, leader id 0 ==&gt; -1 Test (2A): initial election ... 2022/06/26 14:41:55 server 2 成为 candidate, currentTerm 1 2022/06/26 14:41:55 === Candidate 2 开始发送 RequestVote, currentTerm 1 === 2022/06/26 14:41:55 server 0 (term 0 voteFor -1) 收到 candidiate 2 (term 1 candidateId 2) 的RequestVote 2022/06/26 14:41:55 server 0 投票给 server 2 2022/06/26 14:41:55 server 1 (term 0 voteFor -1) 收到 candidiate 2 (term 1 candidateId 2) 的RequestVote 2022/06/26 14:41:55 server 1 投票给 server 2 2022/06/26 14:41:55 server 2 成为 leader, currentTerm 1 ... ... Passed -- 3.1 3 52 12924 0 --- PASS: TestInitialElection2A (3.07s) ... 2022/06/26 14:41:59 === Leader 1 开始发送 AppendEntries === 2022/06/26 14:41:59 === server 2 收到来自 Leader 1 的 AppendEntries === 2022/06/26 14:41:59 === server 0 收到来自 Leader 1 的 AppendEntries === ... 2022/06/26 14:42:01 === Leader 1 开始发送 AppendEntries === 2022/06/26 14:42:01 server 2 成为 candidate, currentTerm 10 2022/06/26 14:42:01 === Candidate 2 开始发送 RequestVote, currentTerm 10 === 2022/06/26 14:42:01 server 0 (term 7 voteFor 0) 收到 candidiate 2 (term 10 candidateId 2) 的RequestVote 2022/06/26 14:42:01 server 0 成为 follower，currentTerm 7 ==&gt; 10, leader id 0 ==&gt; -1 2022/06/26 14:42:01 server 0 投票给 server 2 2022/06/26 14:42:01 server 2 成为 leader, currentTerm 10 ... ... Passed -- 4.5 3 100 17870 0 --- PASS: TestReElection2A (4.50s) PASS 开始写Lab时，首先进行任务拆分： 结构定义 新建一个实例时，新建核心循环线程 实现仅进行心跳检测的AppendEntries RPC，包括发送和接受 判断超过election timeout后，开启election candidate成功选为leader的情况 candidate收到其他candidate当选的情况 candidate进入新一轮选举的情况 随机化的election timeouts (按照lab要求要略微大于 150-300ms 这个范围) 实现 GetState ，使其能够获取实例的状态 实现细节1 结构定义实际开发中，定义了身份状态（没想到golang用这种方式定义枚举） const ( LEADER int = 0 CANDIDATE = 1 FOLLOWER = 2 ) 结构体主要见论文 Figure 2 中提到的字段。除此以外，Raft类里我还定义了如下字段。后续Lab中估计还会有新的 ... state int electionTimeout int heartBeat bool takenVote bool voteCnt int ... 解释一下，state是身份状态，electionTimeout是记录的延迟时间，voteCnt是计票是用的。重点说下heartBeat和takenVote，这两个代表的是一个election timeout内有没有收到心跳，有没有发送投票。按照规则，如果election timeout到了，没有收到心跳并不代表一定转为candidate，发送投票了也是就继续保留为follower。自己实现时候一开始漏了这个点，结果就是server不断地从follower变为candidate再变为follower，永远选不出leader。 2 核心流程核心流程的入口是在新建实例（Make方法）时，新起的循环线程，只要不kill都会跑下去，我把它命名为mainFlow。在这里，leader做的事情是发送AppendEntries，Candidate做的事情是发送RequestVote（如果到了electionTimeout还是Candidate身份，就重新发起一次election），follower做的事情是判断要不要变为Candidate。 func (rf *Raft) mainFlow() &#123; for !rf.killed() &#123; rf.mu.Lock() state := rf.state rf.mu.Unlock() switch state &#123; case LEADER: rf.startAppendEntries() time.Sleep(time.Duration(100) * time.Millisecond) case CANDIDATE: rf.candidateMainFlow() case FOLLOWER: rf.followerMainFlow() &#125; &#125; &#125; 3 RPC发送AppendEntries和发送RequestVote的要点：一个是对peers要异步发送，同步发送信息是不可接受的： for i := range rf.peers &#123; go func(ii int) &#123; // ... &#125;(i) &#125; 在Lab2A中，AppendEntries只包含空的内容，比较好实现。不过要实现对参数和返回值中的term进行判断的逻辑 另一个要点是，我在每次加锁之后都先判断一下state。两段锁之间状态的值是有可能改变的，因此必须加以校验。 4 辅助方法获取election timeout的方法，在转变为follower、candidate时重新获取： func (rf *Raft) genNewElectionTimeout() &#123; r := rand.New(rand.NewSource(time.Now().UnixNano())) const min = 200 const max = 400 electionTimeout := r.Intn(max-min) + min rf.electionTimeout = electionTimeout &#125; 最后有三个实用的方法，会在各处多次被调用：convertToFollower、convertToCandidate、convertToLeader。convertToFollower是需要参数的，需要currentTerm和voteFor，将这两个值更新，然后重新随机生成electionTimeout。convertToCandidate会递增currentTerm，然后给自己投票，重新随机生成electionTimeout。convertToLeader会初始化nextIndex和matchIndex，这两个目前Lab2A没用到，之后再细说。","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"MIT 6.824 Lab2 - Raft - Lab2前准备的笔记","slug":"2022/6.824Lab2-Lab2前准备的笔记","date":"2022-06-12T06:58:41.090Z","updated":"2023-02-23T14:19:33.649Z","comments":true,"path":"2022/06/12/2022/6.824Lab2-Lab2前准备的笔记/","link":"","permalink":"https://juniousy.github.io/2022/06/12/2022/6.824Lab2-Lab2%E5%89%8D%E5%87%86%E5%A4%87%E7%9A%84%E7%AC%94%E8%AE%B0/","excerpt":"","text":"做lab前先整理一下要点和课前提醒，做一个笔记记录，可跳过，主要在实现时对照着看。 Raft概述Raft是一个用来实现分布式一致的协议。Raft is a protocol for implementing distributed consensus. Raft is a consensus algorithm that is designed to be easy to understand. It’s equivalent to Paxos in fault-tolerance and performance. The difference is that it’s decomposed into relatively independent subproblems, and it cleanly addresses all major pieces needed for practical systems. We hope Raft will make consensus available to a wider audience, and that this wider audience will be able to develop a variety of higher quality consensus-based systems than are available today. 主要分为Lead Election和Log Replication阶段 Log Replication 阶段流程概述： 为了commit the log entry，leader node首先向follower nodes复制自己 leader等待大部分node写入entry entry 提交，leader status改变 leader 通知followers entry已经提交了 cluster 的系统状态成为一致(consensus) Lead Election 阶段流程概述： election timeout：时间结束后，follower变为candidate，发起election，发送request vote candidate 被大部分note vote后，变为leader leader向followers发送Append Entries，按 heartbeat timeout 间歇发送 follower也向leaderAppend Entries，作为心跳检测 同时有两个candidate，就重新进入election timeout等待，重新发起election Raft精要内容是论文中的Figure 2。 1. State关于服务器状态的实现 所有服务器 - 持久化状态在进行RPC回复前进行持久化 currentTerm 最近server看到的term （0开始，单调增） voteFor 当前term下被推举的candidate Id log[] log entries （记录条目）。每个entry包含状态机指令，和收到leader发出的entry时的term（起始值为1） 所有服务器 - 可变状态 commitIndex 最近一次被提交的log entry序号（0开始，单调增） lastApplied 最近一次被应用的log entry序号（0开始，单调增） leaders - 可变状态选举后重新初始化 nextIndex[] 对每个server，最近发送的log entry序号（原index+1） matchIndex[] 对每个server，最近知晓的复制成功的log entry序号 2. AppendEntries RPC功能为复制log entries和心跳检测 参数 term leader的term leaderId prevLogIndex prevLogTerm entries 要存的log entries （空为心跳；可能一次传多个） leaderCommit leader的commitIndex 结果 term currentTerm，leader用来更新自己 success 成功时，表示follower包含符合prevLogIndex和prevLogTerm的entry Receiver实现 如果 term &lt; currentTerm , 返回false 如果 prevLogIndex处的entry不匹配prevLogTerm，返回false 如果现有的entry和新的冲突（index一样但是term不一样），从这个entry开始删除到最新。 插入新的entries 如果 leaderCommit &gt; commitIndex，设置 commitIndex = min(leaderCommit, 最新entry的序号) 3. RequestVote RPCcandidate收集选票时触发 参数 term candidate的term candidateId lastLogIndex candidate最后一个log entry的序号 lastLogTerm candidate最后一个log entry的term 结果 term 当前term voteGranted true表示投一票 Receiver实现 如果term &lt; currentTerm 返回false 如果votedFor是空或者是candidateId，然后candidate的log和receiver的log比不滞后，就投票同意 4. Servers的实现规则所有servers 如果 commitIndex &gt; lastApplied：lastApplied+1，将log[lastApplied]应用到状态机中 如果RPC请求或返回包含 term T &gt; currentTerm：将currentTerm设为T，自己转为follower Followers 向candidates和leaders的RPC回复 如果election timeout到了之后，没有收到leader的AppendEntries RPC或收到candidate的投票请求，就自己转为candidate Candidates 转变成candidate，发起选举 currentTerm + 1 投自己一票 重设election timer 向所有其他服务发送RequestVote RPC 如果得到大部分服务的投票，成为leader 如果收到新leader的AppendEntries RPC，成为follower 如果election timeout到了，开始新选举 Leaders 选举一旦完成，向各服务发送新的初始化空AppendEntries RPC（心跳），空闲时也重复发送 如果收到client的命令：在本地log中新增entry，在状态机上应用entry后返回 如果最近的log序号 &gt;= nextIndex，发送AppendExtries RPC时用nextIndex 如果成功：为follower更新nextIndex和matchIndex 如果失败，那么原因为log不一致，降低nextIndex重试 如果存在 N，N &gt; commitIndex，大部分matchIndex[i] &gt;= N，log[N].term == currentTerm，那么，就设置commitIndex = N 注意要点课程提醒的实现中需要注意的点，一个是Figure 2上的要点要逐一实现，如接受非heart beat AppendEntries 时也要进行相应的检查等；第二个是归纳了四种常见的bug。 Bugs1. live locks - 活锁 需要妥善处理好重设election timer。如果AppendEntries已经过时了，就不要重设计时器。开始发起选举时要重设。给其他节点投票时要重设，而不是每次收到投票请求时重设(这样有更多最近记录的节点更有可能选上)。 如果是candidate正在发起选举，但是自己的election timer到时间了，那么就应该开始新的一次选举 如果已经给出投票，然后有新的RequestVote RPC有更高的term，那应该启用这个term，然后处理RPC 2. Incorrect RPC handlers - 错误的PRC处理 Figure 2要点中说的“返回false”，意思是立即返回 如果一个AppendEntries RPC的prevLogIndex比最近的log index要早，那该当做有这个entry但是term不匹配来处理（如返回false） 对prevLogIndex的检查处理，在leader没有送出entries时也要处理 leader的commit index大过自身commit index时，要更新为min(leaderCommit, 最新entry的序号)。如果直接改为leaderCommit，会遇到应用错误的entries的情况。 严格按照要求处理“up-to-date log”。Raft判断的两个log哪个最up-to-date，是通过比较Index和logs中最后的entries的term。如果term最新，那么有最新term的log最up-to-date。如果term一样，那么哪个log的长度（entries的数量）最长就算最up-to-date。 3. Failure to follow The Rules - 没有正确遵守规则Figure 2. 外补充的要点 任何时候发现commitIndex &gt; lastApplied，应该应用一条log entry到状态机。apply要保证只有一个地方去执行。具体来说，要么有一个专门的applier，要么在apply时加锁。 检查commitIndex &gt; lastApplied要么周期性，要么在commitIndex更新之后。 如果leader发出AppendEntries RPC被拒绝时，如果不是因为log不一致，那么应该立即退出，不更新nextIndex。 leader 不能让其他节点在过时的term中更新commitIndex。因此要判断log[N].term == currentTerm。 matchIndex和nextIndex的关系不单纯是matchIndex = nextIndex - 1。nextIndex只是一种乐观的猜测。matchIndex是安全保障，用来做判断。 4. Term Confusion - term混乱在收到旧term的RPC返回时，只在当前term和请求时的term一致时，处理该PRC返回。 更新matchIndex正确的操作是设置为prevLogIndex + len(entries[])，这里面的参数是发起请求时的值。 额外的功能这门课除了核心功能，还要求实现log压缩（section 7），快速log回溯（第8页左上方）。 log压缩主要看Figure 13。 leader发送表示一个snapshot的多个chunk的方式 参数 term leader的term leaderId lastIncludedIndex lastIncludedTerm offset chunk在snapshot文件中的byte位置偏移量 data[] snapshot chunk 的原始数据，从offset开始 done true表示为最后一个chunk 结果 term 当前term Receiver Implementation 如果 term &lt; currentTerm，立即返回 第一个chunk时新建snapshot文件 在给的offset处写入文件 如果done是false，返回并等待更多chunk 保存snapshot文件，其他snapshot文件如果有更小的index，就丢弃 如果已有的log entry和snapshot的最后一个entry有同样的index和term，保留这之后的log entries 丢弃整个log 用snapshot内容重设状态机 注意事项： 在做snapshot时，应用的状态应该对设计raft log中已经有index应用过的状态。也就是说，要么知道snapshot对应的index，要么raft在完成snapshot前不应用新的log entries。 状态和snapshot提交是分开的，所以在此两者之间的崩溃会导致问题，因为此时被snapshot覆盖的log已经丢弃了。解决办法是记录真实的Raft持久化日志第一条内容的index。 快速log回溯 如果follower在log中没有prevLogIndex，那么应该返回conflictIndex = len(log)和conflictTerm = None 如果follower在log中有prevLogIndex，但是term对不上，那么返回值conflictTerm = log[prevLogIndex].Term，然后找第一个entry的term等于conflictTerm，回溯到这个index 收到冲突返回时，leader应该在log中搜索conflictTerm。如果找到了，就把nextIndex设为这个term最后的index之前的那个index。（这句话没看懂，原文If it finds an entry in its log with that term, it should set nextIndex to be the one beyond the index of the last entry in that term in its log.） 接上条，如果没有找到，设置nextIndex = conflictIndex 应用Raft时的注意点Applying client operations - 应用操作服务应该被设计为一个状态机。需要有一个循环去接受用户的操作，和将用户的操作按顺序应用到状态机。这个循环是唯一一处能接触到状态机的地方。 如何知道用户的请求已经完成？在用户操作时，记录当前log的index，一旦在那个index处的操作被标记为已应用，看该处的操作是不是当时的操作，是表示操作成功，否表示操作失败。 Duplicate detection - 重复检测防止应用两次：每个client有一个id，每次请求有一个单调增id。如果相同clinet的相同请求id已经处理过了，就忽略。 其他问题重复Index，死锁 附录： In Search of an Understandable Consensus Algorithm(Extended Version) Students’ Guide to Raft","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"MIT 6.824 Lab1 - Map Reduce","slug":"2022/6.824Lab1","date":"2022-05-30T12:00:00.000Z","updated":"2023-02-23T14:30:07.706Z","comments":true,"path":"2022/05/30/2022/6.824Lab1/","link":"","permalink":"https://juniousy.github.io/2022/05/30/2022/6.824Lab1/","excerpt":"","text":"--- wc test: PASS *** Starting indexer test. --- indexer test: PASS *** Starting map parallelism test. --- map parallelism test: PASS *** Starting reduce parallelism test. --- reduce parallelism test: PASS *** Starting crash test. --- crash test: PASS *** PASSED ALL TESTS 课程要求是不公开自己的代码的，遵守一下规则。这里简单讲讲思路和遇到的问题 实现首先思考一下“要做什么”，所以做了一下功能点的拆分 1. master收到任务（文件名）后负责拆分任务 2. worker向master申请任务（可能是map也可能是reduce） 3. worker map操作，存入中间文件 4. worker reduce操作，读取中间文件，写入文件 5. crash worker 的处理 6. master判断任务是否已经全部完成；worker结束进程 master是有状态的，设计如下： type Master struct &#123; // Your definitions here. files []string //待处理的文件 mapfTasks map[int]int //key为TaskId，value为task状态0 not start 1 in progress 2 success mapfAllSuccess bool // mapre任务是否都完成 nReduce int // reduce任务的数量 reducefTasks map[int]int // key为TaskId，value为task状态0 not start 1 in progress 2 success reducefTaskFileMap map[int][]string // 一个kv表示一个 reduceId 所需要处理所有文件 done bool // 是否完成 mu sync.Mutex // 锁 &#125; 我的实现用了两个rpc： func (m *Master) AskForTask(args struct&#123;&#125;, reply *AskForTaskReply) error func (m *Master) ReportTask(args ReportTaskArgs, replay *struct&#123;&#125;) error 方法AskForTask就是一个worker向master申请任务的过程，对应功能点1、2和6。 功能点1和2好理解。这主要是根据mapfTasks和reducefTasks这两个任务状态表分发，这两个表key是待处理map任务的id，value是任务状态（未开始、进行中、已完成）。 for taskId, status := range m.mapfTasks &#123; if status == 0 &#123; // 分发任务 &#125; &#125; 为什么会有6是因为考虑到worker持续循环调用AskForTask，所以把判断任务全部完成的状态也加在这个方法里，AskForTaskReply会告知worker任务全部完成。 if successCnt == len(m.reducefTasks) &#123; m.done = true &#125; 功能点3、4在worker.go下实现。map操作的核心思想是读取待处理的文件，调用mapf，写入intermedia file，通过rpc方法ReportTask告知master任务已完成。。reduce操作的核心思想是根据master指定的intermedia filename去读取中间文件，然后调用reducef，然后通知任务完成。我这样实现的话，有个关键的点是，map在通知任务完成时，要把中间文件的filename也告诉master，因为这个是reduce任务的来源。 任务5我的做法比较简单，在每次分发任务时都新建一个线程延时等待，go m.waitForMapfSuccess(taskId)。如果等待时间结束，任务还未完成，就把任务从进行中改为未开始。这个做法还是比较粗糙的，不过在这个lab里能够处理。 func (m *Master) waitForMapfSuccess(taskId int) &#123; time.Sleep(10 * time.Second) m.mu.Lock() defer m.mu.Unlock() if m.mapfTasks[taskId] == 1 &#123; //log.Printf(&quot;mapf taskid %v 超时&quot;, taskId) m.mapfTasks[taskId] = 0 &#125; &#125; 问题点环境我在wsl2环境下实现，而golang版本不是Go1.13，而是Go1.18.2。这里还是不建议更改版本，不过因为module相关的一些问题，哪怕我用了go1.13也会触发同样的问题，所以最后我改了下测试文件的编译命令，直接用1.18去跑（逃 ... (cd .. &amp;&amp; GO111MODULE=off go build $RACE mrmaster.go) || exit 1 (cd .. &amp;&amp; GO111MODULE=off go build $RACE mrworker.go) || exit 1 (cd .. &amp;&amp; GO111MODULE=off go build $RACE mrsequential.go) || exit 1 ... 这个改动应该是不影响在要求的环境下的测试结果的 race加-race被提醒有地方会有问题 WARNING: DATA RACE Write at 0x00c000100220 by goroutine 78: _/home/...../MIT6.824/src/mr.(*Master).AskForTask() /home/...../MIT6.824/src/mr/master.go:75 +0x885 Previous read at 0x00c000100220 by main goroutine: _/home/.....g/MIT6.824/src/mr.(*Master).Done() /home/...../MIT6.824/src/mr/master.go:134 +0xef parallelism测试卡死 只能手动Kill这个问题一开始很困惑，后来发现单跑这个测试是能通过的。然后我发现这个测试会读当前文件夹。而我一开始没有手动删除intermedia file，这个test也不会删之前的intermedia file，我猜测问题点出在这里。事实证明在加入了任务完成后删除intermedia file后，就能通过了。 其他小bug比如重复问题，debug发现是没有等map全部结束就发出去了reduce任务。 ADLER 1 ADVENTURE 12 ADVENTURES 7 AFTER 2 AGREE 16 AGREEMENT 8 ... ADLER 1 ADVENTURE 12 ADVENTURES 7 AFTER 2 AFTER 2 AGREE 16 AGREEMENT 8 还有crash test超时问题，打了日志后再稍微看下代码就发现是小bug，不多赘述。","categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"}]},{"title":"【本科时期文章】Java 同步器核心AQS","slug":"存档/Java-同步器核心AQS","date":"2019-06-14T03:27:12.000Z","updated":"2022-05-30T09:06:50.980Z","comments":true,"path":"2019/06/14/存档/Java-同步器核心AQS/","link":"","permalink":"https://juniousy.github.io/2019/06/14/%E5%AD%98%E6%A1%A3/Java-%E5%90%8C%E6%AD%A5%E5%99%A8%E6%A0%B8%E5%BF%83AQS/","excerpt":"juc(java.util.concurrent) 基于 AQS （ AbstractQueuedSynchronizer ）框架构建锁机制。本文将介绍AQS是如何实现共享状态同步功能，并在此基础上如何实现同步锁机制。","text":"juc(java.util.concurrent) 基于 AQS （ AbstractQueuedSynchronizer ）框架构建锁机制。本文将介绍AQS是如何实现共享状态同步功能，并在此基础上如何实现同步锁机制。 AbstractQueuedSynchronizerCLH同步队列AQS如其名所示，使用了队列。当共享资源（即多个线程竞争的资源）被某个线程占有时，其他请求该资源的线程将会阻塞，进入CLH同步队列。 队列的节点为AQS内部类Node。Node持有前驱和后继，因此队列为双向队列。有如下状态： SIGNAL 后继节点阻塞(park)或即将阻塞。当前节点完成任务后要唤醒(unpark)后继节点。 CANCELLED 节点从同步队列中取消 CONDITION 当前节点进入等待队列中 PROPAGATE 表示下一次共享式同步状态获取将会无条件传播下去 0 其他 AQS通过头尾指针来管理同步队列，同时实现包括获取锁失败的线程进行入队，释放锁时唤醒对同步队列中的线程。未获取到锁的线程会创建节点线程安全（compareAndSetTail）的加入队列尾部。同步队列遵循FIFO，首节点是获取同步状态成功的节点。 获取锁未获取到锁（tryAcquire失败）的线程将创建一个节点，设置到尾节点。 public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; //创建节点至尾节点 private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 如果compareAndSetTail失败或者队列里没有节点 enq(node); return node; &#125; enq是一个CAS的入队方法： private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; acquireQueued方法的作用是获取锁。 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // 获取锁成功 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 获取失败则阻塞 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 释放锁首节点的线程在释放锁时，将会唤醒后继节点。而后继节点将会在获取锁成功时将自己设置为首节点。 public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) // 唤醒后继节点 unparkSuccessor(h); return true; &#125; return false; &#125; 响应中断式获取锁可响应中断式锁可调用方法lock.lockInterruptibly();而该方法其底层会调用AQS的acquireInterruptibly方法。 public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg); &#125; private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 唯一的区别是当parkAndCheckInterrupt返回true时即线程阻塞时该线程被中断，代码抛出被中断异常。 throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 超时等待获取锁通过调用lock.tryLock(timeout,TimeUnit)方式达到超时等待获取锁的效果，调用AQS的方法tryAcquireNanos()。 public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout); &#125; tongbuqi private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (nanosTimeout &lt;= 0L) return false; final long deadline = System.nanoTime() + nanosTimeout; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return true; &#125; // 计算等待时间 nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) return false; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 共享锁的获取最后看下共享锁的获取。 public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) //获取锁失败时调用 doAcquireShared(arg); &#125; private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); // 当tryAcquireShared返回值&gt;=0时取得锁 if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 队列外成员变量AQ还有state成员变量，volatile int类型，用于同步线程之间的共享状态。当state&gt;0时表示已经获取了锁，对于重入锁来说state值即重入数，当state = 0时表示释放了锁。具体说明见下面各同步器的实现。 实现同步器每一种同步器都通过实现tryacquire（包括如tryAcquireShared之类的方法）、tryRelease来实现同步功能。 ReentrantLock主要看获取锁的过程非公平锁获取锁： final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //如果当前重进入数为0,说明有机会取得锁 if (c == 0) &#123; //抢占式获取锁 compareAndSetState是原子方法 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //如果当前线程本身就持有锁，那么叠加重进入数，并且继续获得锁 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; //以上条件都不满足，那么线程进入等待队列。 return false; &#125; 公平锁获取锁类似： protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 区别之处，非抢占式 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; Semaphore以state作为信号量使用，例子： final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; //剩下多少许可资源 if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125; &#125; CountDownLatch以state作为计数器，state为0时等待结束： public void await() throws InterruptedException &#123; //阻塞直到state为0 sync.acquireSharedInterruptibly(1); &#125; 用同步器方法减少state public void countDown() &#123; sync.releaseShared(1); &#125; protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125;","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://juniousy.github.io/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://juniousy.github.io/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"【本科时期文章】Redis的数据结构与编码","slug":"存档/Redis的数据结构与编码","date":"2019-06-05T07:42:40.000Z","updated":"2022-05-30T09:06:50.982Z","comments":true,"path":"2019/06/05/存档/Redis的数据结构与编码/","link":"","permalink":"https://juniousy.github.io/2019/06/05/%E5%AD%98%E6%A1%A3/Redis%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%96%E7%A0%81/","excerpt":"类型 编码方式 数据结构 string raw 动态字符串编码 embstr 优化内存分配的字符串编码 int 整数编码 hash hashtable 散列表编码 ziplist 压缩列表编码 list linkedlist 双向链表编码 ziplist 压缩列表编码 quicklist 3.2版本新的列表编码 set hashtable 散列表编码 intset 整数集合编码 zset skiplist 跳跃表编码 ziplist 压缩列表编码","text":"类型 编码方式 数据结构 string raw 动态字符串编码 embstr 优化内存分配的字符串编码 int 整数编码 hash hashtable 散列表编码 ziplist 压缩列表编码 list linkedlist 双向链表编码 ziplist 压缩列表编码 quicklist 3.2版本新的列表编码 set hashtable 散列表编码 intset 整数集合编码 zset skiplist 跳跃表编码 ziplist 压缩列表编码 字符串结构Redis没有采用原生C语言的字符串类型，而是自己实现了字符串结构，内部简单动态字符串(simple dynamic string，SDS)。特点如下： O(1)时间复杂度获取字符串长度、已用长度、未用长度 可用于保存字节数组，支持安全的二进制数据存储 内部实现空间预分配机制，降低内存内存再分配次数 惰性删除机制，字符串缩减后的空间不释放，作为预分配空间保留 对于string， int：8个字节的长整型 embstr：小于等于39个字节的字符串 raw：大于39个字节的字符串，即用简单动态字符串（SDS）存储 embstr 编码的优化之处在于将创建字符串对象所需的内存分配次数从 raw 编码的两次降低为一次，mbstr 编码的字符串对象的所有数据都保存在一块连续的内存里面，redisObject 结构(type, encoding…)和 sdshdr 结构(free, len, buf)都放在一起embstr 编码的字符串对象实际上是只读的： 当我们对 embstr 编码的字符串对象执行任何修改命令时， 程序会先将对象的编码从 embstr 转换成 raw ， 然后再执行修改命令； 因为这个原因， embstr 编码的字符串对象在执行修改命令之后， 总会变成一个 raw 编码的字符串对象。 ziplist 压缩列表hash、list、zset中，如果所有值小于hash_max_ziplist_value （默认值为 64 ），且元素个数小于 hash_max_ziplist_entries （默认值为 512 ）时使用ziplist编码。 ziplist编码的主要目的是为了节约内存，因此所有数据都是采用线性连续的内存结构。结构字段含义： zlbytes：整个压缩列表所占字节长度。int-32，长度4字节。 zltail：距离尾节点的偏移量。int-32，长度4字节。 zllen：int-16，长度2字节。 entry：具体的节点： prev_entry_bytes_length：记录前一个节点所占空间 encoding：标示当前节点编码和长度 contents：保存节点的值 zlend：记录列表结尾，占一个字节 从上可以看出存在双向链表结构，以O(1)时间复杂度入队和出队。而新增删除操作涉及内存重新分配和释放。 hashtableRedis 使用的hash算法是 MurmurHash2 ，解决冲突的方式是链地址法。程序总是将新节点添加到链表的表头位置（复杂度为 O(1)）， 排在其他已有节点的前面。按2的幂rehash。 linkedlistRedis 的链表实现的特性可以总结如下： 双端： langfei链表节点带有 prev 和 next 指针， 获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环： 表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ， 对链表的访问以 NULL 为终点。 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针， 程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器： 程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数， 程序获取链表中节点数量的复杂度为 O(1) 。 多态： 链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数， 所以链表可以用于保存各种不同类型的值。 intset存储有序、不重复的整数集。集合只包含整数且长度不超过set-max-intset-entries intset对写入整数进行排序，通过O(lgn)时间复杂度实现查找和去重操作。字段含义： encoding：整数表示类型，根据集合内最长整数值确定类型，整数类型划分为int-16，int-32，int-64 length：表示集合元素个数 contents：整数数组，按从小到达顺序排列 尽量保证整数范围一致，防止个别大整数触发集合升级操作，产生内存浪费。 skiplist过在每个节点中维持多个指向其他节点的指针， 从而达到快速访问节点的目的。跳跃表支持平均 O(log N) 最坏 O(N) 复杂度的节点查找， 还可以通过顺序性操作来批量处理节点。 ObjectRedis 中的每个对象都由一个 redisObject 结构表示， 该结构中和保存数据有关的三个属性分别是 type 属性、 encoding 属性和 ptr 属性。对象的 type 属性记录了对象的类型。对象的 ptr 指针指向对象的底层实现数据结构， 而这些数据结构由对象的 encoding 属性决定。 因为 C 语言并不具备自动的内存回收功能， 所以 Redis 在自己的对象系统中构建了一个引用计数（reference counting）技术实现的内存回收机制， 通过这一机制， 程序可以通过跟踪对象的引用计数信息， 在适当的时候自动释放对象并进行内存回收。由redisObject 结构的 refcount 属性记录： 在创建一个新对象时， 引用计数的值会被初始化为 1 ； 当对象被一个新程序使用时， 它的引用计数值会被增一； 当对象不再被一个程序使用时， 它的引用计数值会被减一； 当对象的引用计数值变为 0 时， 对象所占用的内存会被释放。 redisObject 结构包含的最后一个属性为 lru 属性， 该属性记录了对象最后一次被命令程序访问的时间。OBJECT IDLETIME 命令可以打印出给定键的空转时长， 这一空转时长就是通过将当前时间减去键的值对象的 lru 时间计算得出的。","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://juniousy.github.io/tags/Redis/"}]},{"title":"【本科时期文章】从Redis I/O多路复用到Java NIO Selector","slug":"存档/从Redis-I-O多路复用到Java-NIO-Selector","date":"2019-06-04T07:19:21.000Z","updated":"2022-05-30T09:06:50.983Z","comments":true,"path":"2019/06/04/存档/从Redis-I-O多路复用到Java-NIO-Selector/","link":"","permalink":"https://juniousy.github.io/2019/06/04/%E5%AD%98%E6%A1%A3/%E4%BB%8ERedis-I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E5%88%B0Java-NIO-Selector/","excerpt":"Redis的I/O多路复用架构Redis的一大特点就是单线程架构。单线程架构既避免了多线程可能产生的竞争问题，又避免了多线程的频繁上下文切换问题，是Redis高效率的保证。","text":"Redis的I/O多路复用架构Redis的一大特点就是单线程架构。单线程架构既避免了多线程可能产生的竞争问题，又避免了多线程的频繁上下文切换问题，是Redis高效率的保证。 对于网络I/O操作，Redis基于 Reactor 模式可以用单个线程处理多个Socket。内部实现为使用文件事件处理器(file event handler)进行网络事件处理器，这个文件事件处理器是单线程的。文件事件处理器采用 I/O 多路复用机制(multiplexing)同时监听多个 socket。产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。操作包括应答（accept）、读取（read）、写入（write）、关闭（close）等。文件事件处理器的结构包含 4 个部分： 多个 socket I/O 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）连接应答处理器会创建一个能与客户端通信的 socket01，通过这个返回结果给客户端。Redis单线程的核心就是I/O 多路复用程序。 I/O多路复用（IO Multiplexing）有时也称为异步阻塞IO，是一种事件驱动的I/O模型。单个I/O操作在一般情况下往往不能直接返回，传统的阻塞 I/O 模型会阻塞直到系统内核返回数据。而在 I/O 多路复用模型中，系统调用select/poll/epoll 函数会不断的查询所监测的 socket 文件描述符，查看其中是否有 socket 准备好读写了，如果有，那么系统就会通知用户进程。 Redis 的 I/O 多路复用程序的所有功能都是通过包装常见的 select 、 epoll 、 evport 和 kqueue 这些 I/O 多路复用函数库来实现的， 每个 I/O 多路复用函数库在 Redis 源码中都对应一个单独的文件。 以ae_select.c实现的封装select方法为例。select方法定义如下所示，检测是否可读、可写、异常，返回准备完毕的descriptors个数。 extern int select (int __nfds, fd_set *__restrict __readfds, fd_set *__restrict __writefds, fd_set *__restrict __exceptfds, struct timeval *__restrict __timeout); Redis封装首先通过aeApiCreate初始化 rfds 和 wfds，注册到aeEventLoop中去。 static int aeApiCreate(aeEventLoop *eventLoop) &#123; aeApiState *state = zmalloc(sizeof(aeApiState)); if (!state) return -1; FD_ZERO(&amp;state-&gt;rfds); FD_ZERO(&amp;state-&gt;wfds); eventLoop-&gt;apidata = state; return 0; &#125; 而 aeApiAddEvent 和 aeApiDelEvent 会通过 FD_SET 和 FD_CLR 修改 fd_set 中对应 FD 的标志位。 static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) &#123; aeApiState *state = eventLoop-&gt;apidata; if (mask &amp; AE_READABLE) FD_SET(fd,&amp;state-&gt;rfds); if (mask &amp; AE_WRITABLE) FD_SET(fd,&amp;state-&gt;wfds); return 0; &#125; static void aeApiDelEvent(aeEventLoop *eventLoop, int fd, int mask) &#123; aeApiState *state = eventLoop-&gt;apidata; if (mask &amp; AE_READABLE) FD_CLR(fd,&amp;state-&gt;rfds); if (mask &amp; AE_WRITABLE) FD_CLR(fd,&amp;state-&gt;wfds); &#125; aeApiPoll是实际调用 select 函数的部分，其作用就是在 I/O 多路复用函数返回时，将对应的 FD 加入 aeEventLoop 的 fired 数组中，并返回事件的个数： static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) &#123; aeApiState *state = eventLoop-&gt;apidata; int retval, j, numevents = 0; memcpy(&amp;state-&gt;_rfds,&amp;state-&gt;rfds,sizeof(fd_set)); memcpy(&amp;state-&gt;_wfds,&amp;state-&gt;wfds,sizeof(fd_set)); retval = select(eventLoop-&gt;maxfd+1, &amp;state-&gt;_rfds,&amp;state-&gt;_wfds,NULL,tvp); if (retval &gt; 0) &#123; for (j = 0; j &lt;= eventLoop-&gt;maxfd; j++) &#123; int mask = 0; aeFileEvent *fe = &amp;eventLoop-&gt;events[j]; if (fe-&gt;mask == AE_NONE) continue; if (fe-&gt;mask &amp; AE_READABLE &amp;&amp; FD_ISSET(j,&amp;state-&gt;_rfds)) mask |= AE_READABLE; if (fe-&gt;mask &amp; AE_WRITABLE &amp;&amp; FD_ISSET(j,&amp;state-&gt;_wfds)) mask |= AE_WRITABLE; eventLoop-&gt;fired[numevents].fd = j; eventLoop-&gt;fired[numevents].mask = mask; numevents++; &#125; &#125; return numevents; &#125; epoll函数的封装类似。区别在于 epoll_wait 函数返回时并不需要遍历所有的 FD 查看读写情况；在 epoll_wait 函数返回时会提供一个 epoll_event 数组，其中保存了发生的 epoll 事件（EPOLLIN、EPOLLOUT、EPOLLERR 和 EPOLLHUP）以及发生该事件的 FD。Redis封装的调用只需要将epoll_event数组中存储的信息加入eventLoop的 fired 数组中，将信息传递给上层模块： static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) &#123; aeApiState *state = eventLoop-&gt;apidata; int retval, numevents = 0; retval = epoll_wait(state-&gt;epfd,state-&gt;events,eventLoop-&gt;setsize, tvp ? (tvp-&gt;tv_sec*1000 + tvp-&gt;tv_usec/1000) : -1); if (retval &gt; 0) &#123; int j; numevents = retval; for (j = 0; j &lt; numevents; j++) &#123; int mask = 0; struct epoll_event *e = state-&gt;events+j; if (e-&gt;events &amp; EPOLLIN) mask |= AE_READABLE; if (e-&gt;events &amp; EPOLLOUT) mask |= AE_WRITABLE; if (e-&gt;events &amp; EPOLLERR) mask |= AE_WRITABLE; if (e-&gt;events &amp; EPOLLHUP) mask |= AE_WRITABLE; eventLoop-&gt;fired[j].fd = e-&gt;data.fd; eventLoop-&gt;fired[j].mask = mask; &#125; &#125; return numevents; &#125; 当Socket变得可读时（客户端对Socket执行 write 操作，或者执行 close 操作）， 或者有新的可应答（acceptable）Socket出现时（客户端对服务器的监听Socket执行 connect 操作），Socket产生 AE_READABLE 事件。而当Socket变得可写时（客户端对Socket执行 read 操作）， Socket产生 AE_WRITABLE 事件。I/O 多路复用程序允许服务器同时监听Socket的 AE_READABLE 事件和 AE_WRITABLE 事件， 如果一个Socket同时产生了这两种事件， 那么文件事件分派器会优先处理 AE_READABLE 事件， 等到 AE_READABLE 事件处理完之后， 才处理 AE_WRITABLE 事件。换句话说， 如果一个Socket又可读又可写的话， 那么服务器将先读Socket， 后写Socket。 Java NIO SelectorJava中也有I/O多路复用的方式，例子为NIO的Selector。selector的创建方式为调用Selector类的静态方法，由SelectorProvider提供：Selector selector = Selector.open(); public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector(); &#125; SelectorProvider是单例模式，Linux默认提供EPollSelectorProvider，即提供的Selector为EPollSelectorImpl。 public static SelectorProvider provider() &#123; synchronized (lock) &#123; if (provider != null) return provider; return AccessController.doPrivileged( new PrivilegedAction&lt;SelectorProvider&gt;() &#123; public SelectorProvider run() &#123; if (loadProviderFromProperty()) return provider; if (loadProviderAsService()) return provider; provider = sun.nio.ch.DefaultSelectorProvider.create(); return provider; &#125; &#125;); &#125; &#125; //..... /** * Returns the default SelectorProvider. */ public static SelectorProvider create() &#123; String osname = AccessController .doPrivileged(new GetPropertyAction(&quot;os.name&quot;)); if (osname.equals(&quot;SunOS&quot;)) return createProvider(&quot;sun.nio.ch.DevPollSelectorProvider&quot;); if (osname.equals(&quot;Linux&quot;)) return createProvider(&quot;sun.nio.ch.EPollSelectorProvider&quot;); return new sun.nio.ch.PollSelectorProvider(); &#125; 调用系统Epoll方法的地方在EPollArrayWrapper类的poll方法中，该类由EPollSelectorImpl持有： int poll(long timeout) throws IOException &#123; updateRegistrations(); updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd); for (int i=0; i&lt;updated; i++) &#123; if (getDescriptor(i) == incomingInterruptFD) &#123; interruptedIndex = i; interrupted = true; break; &#125; &#125; return updated; &#125; Selector使用中需要绑定Channel。以ServerSocketChannel为例： ServerSocketChannel serverSocket = ServerSocketChannel.open(); serverSocket.bind(new InetSocketAddress(&quot;localhost&quot;, 5454)); serverSocket.configureBlocking(false); serverSocket.register(selector, SelectionKey.OP_ACCEPT); 注册时会调用Selector的回调方法register，生成SelectionKey。 protected final SelectionKey register(AbstractSelectableChannel ch, int ops, Object attachment) &#123; if (!(ch instanceof SelChImpl)) throw new IllegalSelectorException(); SelectionKeyImpl k = new SelectionKeyImpl((SelChImpl)ch, this); k.attach(attachment); synchronized (publicKeys) &#123; implRegister(k); &#125; k.interestOps(ops); return k; &#125; 最后在使用时根据SelectionKeys遍历查看状态。可以通过监听的事件有： Connect – OP_CONNECT client尝试连接 Accept – OP_ACCEPT server端接受连接 Read – OP_READ server端可以开始从channel里读取 Write – OP_WRITE server端可以向channel里写 使用方式类似： while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectedKeys.iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); if (key.isAcceptable()) &#123; register(selector, serverSocket); &#125; if (key.isReadable()) &#123; answerWithEcho(buffer, key); &#125; iter.remove(); &#125; &#125; Selector的wakeup()方法主要作用是解除阻塞在Selector.select()/select(long)上的线程，立即返回，调用了本地的中断方法。可以在注册了新的channel或者事件、channel关闭，取消注册时使用，或者优先级更高的事件触发（如定时器事件），希望及时处理。 通过NIO的I/O多路复用方式可以节约线程资源，提高网络I/O效率。 参考 Redis 设计与实现-文件事件 Redis 和 I/O 多路复用 Introduction to the Java NIO Selector","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://juniousy.github.io/tags/Java/"},{"name":"Redis","slug":"Redis","permalink":"https://juniousy.github.io/tags/Redis/"}]},{"title":"【本科时期文章】LeetCode 23. Merge k Sorted Lists","slug":"存档/LeetCode-23-Merge-k-Sorted-Lists","date":"2019-05-17T08:53:24.000Z","updated":"2022-05-30T12:37:14.956Z","comments":true,"path":"2019/05/17/存档/LeetCode-23-Merge-k-Sorted-Lists/","link":"","permalink":"https://juniousy.github.io/2019/05/17/%E5%AD%98%E6%A1%A3/LeetCode-23-Merge-k-Sorted-Lists/","excerpt":"ProblemMerge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity. Example: Input: [ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6 ] Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6","text":"ProblemMerge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity. Example: Input: [ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6 ] Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 Solution使用priority queue /* * @lc app=leetcode id=23 lang=java * * [23] Merge k Sorted Lists */ /** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */ class Solution &#123; public ListNode mergeKLists(ListNode[] lists) &#123; if (lists == null || lists.length == 0) return null; PriorityQueue&lt;ListNode&gt; pQueue = new PriorityQueue&lt;&gt;((a, b) -&gt; a.val - b.val); ListNode dummy = new ListNode(0); ListNode cur = dummy; for (ListNode node:lists) &#123; if (node == null) continue; pQueue.offer(node); &#125; while(!pQueue.isEmpty()) &#123; cur.next = pQueue.poll(); cur = cur.next; if (cur.next != null) pQueue.offer(cur.next); &#125; return dummy.next; &#125; &#125; Time complexity: O(nlogk) Space complexity: O(k)","categories":[{"name":"刷题","slug":"刷题","permalink":"https://juniousy.github.io/categories/%E5%88%B7%E9%A2%98/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://juniousy.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【本科时期文章】ThreadLocal","slug":"存档/ThreadLocal","date":"2019-03-24T14:12:30.000Z","updated":"2022-05-30T09:06:50.983Z","comments":true,"path":"2019/03/24/存档/ThreadLocal/","link":"","permalink":"https://juniousy.github.io/2019/03/24/%E5%AD%98%E6%A1%A3/ThreadLocal/","excerpt":"ThreadLocal的作用并不是解决多线程共享变量的问题，而是存储那些线程间隔离，但在不同方法间共享的变量。这是线程安全的一种无同步方案，另一种是无同步方案是幂等的可重入代码。 下面先模拟一个基本的ThreadLocal存储User id的模型，然后解析原理。","text":"ThreadLocal的作用并不是解决多线程共享变量的问题，而是存储那些线程间隔离，但在不同方法间共享的变量。这是线程安全的一种无同步方案，另一种是无同步方案是幂等的可重入代码。 下面先模拟一个基本的ThreadLocal存储User id的模型，然后解析原理。 示例import java.util.concurrent.atomic.AtomicInteger; public class ThreadLocalTest &#123; //工作线程 class Worker implements Runnable &#123; ThreadLocal&lt;Integer&gt; userId = ThreadLocal.withInitial(() -&gt; 1); UserRepo userRepo; Worker(UserRepo userRepo) &#123; this.userRepo = userRepo; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; handler(); try &#123; Thread.sleep(30); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void handler() &#123; userId.set(userRepo.getUserId()); System.out.println(Thread.currentThread().getName() + &quot; userId: &quot; + userId.get()); &#125; &#125; //模拟拿自增user id class UserRepo &#123; private AtomicInteger incrUserId = new AtomicInteger(1); private Integer getUserId() &#123; return incrUserId.getAndIncrement(); &#125; &#125; private void test() &#123; UserRepo userRepo = new UserRepo(); for (int i = 0; i &lt; 15; i++) &#123; new Thread(new Worker(userRepo)).start(); &#125; &#125; public static void main(String[] args) &#123; ThreadLocalTest test = new ThreadLocalTest(); test.test(); &#125; &#125; 结果如下 ........(上略) Thread-13 userId: 135 Thread-0 userId: 136 Thread-2 userId: 137 Thread-1 userId: 138 Thread-4 userId: 139 Thread-5 userId: 140 Thread-3 userId: 141 Thread-6 userId: 142 Thread-7 userId: 143 Thread-9 userId: 144 Thread-10 userId: 145 Thread-11 userId: 146 Thread-8 userId: 147 Thread-12 userId: 149 Thread-14 userId: 148 Thread-13 userId: 150 原理核心是ThreadLocal的内部静态类ThreadLocalMap。map的key是ThreadLocal对象，value是和ThreadLocal对象有关联的值。 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; 注意内部Entry是WeakReference的，原因是出于性能考虑。由于不是强联系，所以其他正在使用ThreadLocal的线程，不会妨碍gc那些来自同一个ThreadLocal的终止后的线程的变量，简单来讲就是待gc的变量会被正确gc。 在ThreadLocalMap 的 remove 方法中，除了讲entry的引用设为null以外，还调用了一个expungeStaleEntry方法： if (e.get() == key) &#123; e.clear(); expungeStaleEntry(i); return; &#125; 其中会将所有键为 null 的 Entry 的值设置为 null，这样可以防止内存泄露，已经不再被使用且已被回收的 ThreadLocal 对象对应的Entry也会被gc清除： if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; 在同样的还有rehash, resize方法方法中，也有类似的设置value为null的操作。 在创建线程时，该线程持有threadLocals。这个引用是在ThreadLocal的createMap方法中设定的，否则为null。 void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; 调用ThreadLocalMap的构造方法： ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125; 再返回来看ThreadLocal就很好理解了get方法： public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); //获取当前线程的ThreadLocalMap if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); //从map中取值，key就是当前ThreadLocal对象 if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; set方法： public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); //获取当前线程的ThreadLocalMap if (map != null) map.set(this, value); //向map中存值，key就是当前ThreadLocal对象 else createMap(t, value); &#125; 应用很常见的应用在Session中存储数据。一个Session对应一个线程，对应一组线程内方法间的共享变量，这些变量都可以由ThreadLocal存储。 参考下结合ThreadLocal来看spring事务源码，感受下清泉般的洗涤！，可以看到在Spring事务中，也有类似ThreadLocal的操作，将数据库connection绑定到当前线程，使用的也是一个map。","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://juniousy.github.io/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://juniousy.github.io/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"【本科时期文章】Java Collection笔记","slug":"存档/Java-Collection笔记","date":"2019-03-20T12:19:10.000Z","updated":"2022-05-30T09:06:50.980Z","comments":true,"path":"2019/03/20/存档/Java-Collection笔记/","link":"","permalink":"https://juniousy.github.io/2019/03/20/%E5%AD%98%E6%A1%A3/Java-Collection%E7%AC%94%E8%AE%B0/","excerpt":"这是自己整理的一些Collection的要点笔记，比较零碎，可能可读性不是很强。有新内容时会进行补充。Java Collection框架： Set , HashSet TreeSet(实现SortedSet) SortedSet List , LinkedList ArrayList Queue, PriorityQueue Dequeue Map , HashMap TreeMap(实现SortedMap) SortedMap","text":"这是自己整理的一些Collection的要点笔记，比较零碎，可能可读性不是很强。有新内容时会进行补充。Java Collection框架： Set , HashSet TreeSet(实现SortedSet) SortedSet List , LinkedList ArrayList Queue, PriorityQueue Dequeue Map , HashMap TreeMap(实现SortedMap) SortedMap 基本方法 add(), remove(), contains(), isEmpty(), addAll() hashmap线程不安全，允许存null。实现： 内部有一个静态类Node&lt;K,V&gt; ， 实现 Map.Entry&lt;K,V&gt;，是“ Basic hash bin node”（文档原文）。而TreeNode也是节点的实现，适用于有冲突的情况，冲突后形成的是红黑树。 计算hash值方法：高16位和低16位hashcode异或，降低hash值范围小时的冲突： static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; 用数组存Node，数组长度必须是2的幂 transient Node&lt;K,V&gt;[] table; 缓存entrySet transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; 取：按hash值作为数组下标去取Node。下标是(tab.length - 1) &amp; hash。 由于桶的长度是2的n次方，这么做其实是等于 一个模运算。比如hash是31(11111)，length是4(100)，-1后是11，与运算后是3(11)，就是取模。如果有冲突了，则有多个Node放在一个桶里，要么顺序查找（链表），要么按TreeNode去取（红黑树）。 public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null;} 6. 存，往数组的`(tab.length - 1) &amp; hash`处放。桶里没有的话则直接放，有的话，找有没有相同的值，有的话替换。加了后如果容量达到threshold就resize(); ```java public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; resize() 方法，初始化数组或扩容。扩容时数组容量扩大到2倍然后ReHash，遍历原Entry数组，把所有的Entry重新Hash到新数组。通过e.hash &amp; (newCap - 1)算出新的数组下标，原因是因为数组全是2的幂，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。然后链表和treenode重新放 HashMap 在第一次 put 时初始化，类似 ArrayList 在第一次 add 时分配空间。在哈希碰撞的链表长度达到TREEIFY_THRESHOLD（默认8)后，会把该链表转变成树结构 concurrenthashmapHashMap允许一个key和value为null，而ConcurrentHashMap不允许key和value为null，如果发现key或者value为null，则会抛出NPE。 和hashmap一样有Node&lt;K,V&gt; sizeCtl：private transient volatile int sizeCtl;这是一个用于同步多个线程的共享变量，如果值为负数，则说明table正在被某个线程初始化或者扩容。如果某个线程想要初始化table或者对table扩容，需要去竞争sizeCtl这个共享变量，获得变量的线程才有许可去进行接下来的操作，没能获得的线程将会一直自旋来尝试获得这个共享变量。获得sizeCtl这个变量的线程在完成工作之后再设置回来，使其他的线程可以走出自旋进行接下来的操作 查询和hashmap差不多，(hashCode &amp; (length - 1))取下标。table数组是被volatile关键字修饰，解决了可见性问题 存要复杂一点。首先计算table下标，下标没数据就通过调用casTabAt方法插入数据。有的话，那么就给该下标处的Node（不管是链表的头还是树的根）加锁插入。 扩容操作比较复杂。扩容操作的条件是如果table过小，并且没有被扩容，那么就需要进行扩容，需要使用transfer方法来将久的记录迁移到新的table中去。整个扩容操作分为两个部分，要用到内部类forwardNode。第一部分是构建一个nextTable,它的容量是原来的两倍，这个操作是单线程完成的。第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。 size()方法，结合baseCount和counterCells数组来得到，通过累计两者的数量即可获得当前ConcurrentHashMap中的记录总量。 HashSet用HashMap实现。(内部：private transient HashMap&lt;E,Object&gt; map;) ArrayListfail fast机制：checkForComodification()方法检查modCount，检查有无结构性的改变，变了抛ConcurrentModificationException。 扩容调Arrays.copyOf(elementData, newCapacity); 内部有迭代器类 Iterator。 LinkedList实现List和Deque（即可以当栈、队列、双向队列使用） 内部是一个双向链表 字段存有 size、 first Node（头节点）、last Node。通过头结点、尾节点可以很快地进行双向入队出队操作。随机存储效率不如ArrayList，要遍历节点。按下标读取时，会按照size，判断是链表前半段还是后半段，根据这个从头或尾节点开始遍历。 和ArrayDeque的区别之一：LinkedList可以存null，而ArrayDeque不能存null。这点在写算法题的时候可以注意一下。 ArrayDeque转一张表整理方法。一套接口遇到失败就会抛出异常，另一套遇到失败会返回特殊值。| Queue Method | Equivalent Deque Method | 说明 || ———— | ———————– | ————————————– || add(e) | addLast(e) | 向队尾插入元素，失败则抛出异常 || offer(e) | offerLast(e) | 向队尾插入元素，失败则返回false || remove() | removeFirst() | 获取并删除队首元素，失败则抛出异常 || poll() | pollFirst() | 获取并删除队首元素，失败则返回null || element() | getFirst() | 获取但不删除队首元素，失败则抛出异常 || peek() | peekFirst() | 获取但不删除队首元素，失败则返回null | 内部elements数组的容量一定是2的倍数，并且不会满。存数组的head和tail下标，形成一个循环数组，当这两个下标相等时，数组为空。而在添加元素时，如果这两个下标相等，说明数组已满，将容量翻倍。扩容时重置头索引和尾索引，头索引置为0，尾索引置为原容量的值。 CopyOnWriteArrayList线程安全add set之类的操作都是新建一个复制arraylist适用于 读多些少, 并且数据内容变化比较少的场景","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://juniousy.github.io/tags/Java/"}]},{"title":"【本科时期文章】实现LRUCache","slug":"存档/实现LRUCache","date":"2019-03-11T14:57:49.000Z","updated":"2022-05-30T09:06:50.984Z","comments":true,"path":"2019/03/11/存档/实现LRUCache/","link":"","permalink":"https://juniousy.github.io/2019/03/11/%E5%AD%98%E6%A1%A3/%E5%AE%9E%E7%8E%B0LRUCache/","excerpt":"LRU Cache 算法是操作系统在进行内存管理时可以采用的一种页面置换算法。LRU，就是Least Recently Used的简称，这个算法叫做最近最少使用算法。除了在页面置换中可以使用这一算法，其他需要缓存的场景也可以运用这一算法。这一算法的核心目的就是依照程序的就近原则，尽可能在有限的空间内缓存最多以后会使用到的内容。另外，实现这一算法也是一道LeetCode题目。本文就是演示如何使用java语言实现这一算法。","text":"LRU Cache 算法是操作系统在进行内存管理时可以采用的一种页面置换算法。LRU，就是Least Recently Used的简称，这个算法叫做最近最少使用算法。除了在页面置换中可以使用这一算法，其他需要缓存的场景也可以运用这一算法。这一算法的核心目的就是依照程序的就近原则，尽可能在有限的空间内缓存最多以后会使用到的内容。另外，实现这一算法也是一道LeetCode题目。本文就是演示如何使用java语言实现这一算法。 LinkedHashMap实现LinkedHashMap是最容易的实现方式，因为它内部的实现方式很贴合这一应用，至于为什么下面会有介绍。LinkedHashMap和普通的HashMap不同的地方在于，它保存了迭代顺序，该迭代顺序可以是插入顺序或者是访问顺序。而LRU要求最近读取过得内容有最高的缓存优先度，也就是按照访问顺序来进行迭代。而通过重写removeEldestEntry方法可以让LinkedHashMap保留有限多的数据，删除缓存中不需要的数据。 //简易实现 class LRUCache&lt;K, V&gt; &#123; private static final float loadFactor = 0.75f; private int capacity; private final LinkedHashMap&lt;K, V&gt; map; public LRUCache(int capacity) &#123; if (capacity &lt; 0) &#123; capacity = 0; &#125; this.capacity = capacity; //构造函数参数分别是initialCapacity、loadFactor、accessOrder，accessOrder为true即按访问顺序迭代 map = new LinkedHashMap&lt;K, V&gt;(0, loadFactor, true)&#123; @Override protected boolean removeEldestEntry(Entry eldest) &#123; return size() &gt; LRUCache.this.capacity; &#125; &#125;; &#125; public final V get(K key) &#123; return map.get(key); &#125; public final void put(K key, V value) &#123; map.put(key, value); &#125; &#125; HashMap + 双向链表实现之所以LinkedHashMap能保有这样的性质，是因为它内部的实现是依托了HashMap和双向链表，因此不用LinkedHashMap我们也能实现LRUCache算法。 基本框架 public class LRUCache&lt;K, V&gt; &#123; private int capacity; private HashMap&lt;K, Node&lt;K, V&gt;&gt; map; private Node&lt;K, V&gt; head; private Node&lt;K, V&gt; tail; public LRUCache(int capacity) &#123; this.capacity = capacity; map = new HashMap&lt;&gt;(capacity); head = new Node&lt;&gt;(); tail = new Node&lt;&gt;(); head.next = tail; tail.pre = head; &#125; class Node&lt;K, V&gt; &#123; K key; V value; Node&lt;K, V&gt; pre; Node&lt;K, V&gt; next; &#125; &#125; 公用方法 private void raiseNode(Node&lt;K, V&gt; node) &#123; if (node.pre == head) &#123; return; &#125; Node&lt;K, V&gt; pre = node.pre; Node&lt;K, V&gt; next = node.next; pre.next = next; next.pre = pre; setFirst(node); &#125; private void setFirst(Node&lt;K, V&gt; node) &#123; Node&lt;K, V&gt; first = head.next; head.next = node; node.pre = head; first.pre = node; node.next = first; &#125; get方法，从map里拿Value，同时将它置为链表头 public V get(K key) &#123; if (!map.containsKey(key)) &#123; return null; &#125; Node&lt;K, V&gt; node = map.get(key); raiseNode(node); return node.value; &#125; save方法，如果缓存已满，删除链表尾的值，再添加新的值到链表头 public void save(K key, V value) &#123; if (map.containsKey(key)) &#123; updateNode(key, value); &#125; else &#123; insertNode(key, value); &#125; &#125; private void updateNode(K key, V value) &#123; Node node = map.get(key); node.value = value; raiseNode(node); &#125; private void insertNode(K key, V value) &#123; if (isFull()) &#123; removeLast(); &#125; Node node = new Node(); node.key = key; node.value = value; setFirst(node); map.put(key, node); &#125; private boolean isFull() &#123; return map.size() &gt;= capacity; &#125; 测试 import org.junit.Test; public class LRUCacheTest &#123; LRUCache&lt;Integer, Integer&gt; cache = new LRUCache(3); @Test public void test() &#123; cache.save(1, 7); cache.save(2, 0); cache.save(3, 1); cache.save(4, 2); assert 0 == cache.get(2); assert null == cache.get(7); cache.save(5, 3); assert 0 == cache.get(2); cache.save(6, 4); assert null == cache.get(4); &#125; &#125; //head -&gt; 7 -&gt; tail //head -&gt; 0 -&gt; 7 -&gt; tail //head -&gt; 1 -&gt; 0 -&gt; 7 -&gt; tail //head -&gt; 2 -&gt; 1 -&gt; 0 -&gt; tail //head -&gt; 3 -&gt; 2 -&gt; 1 -&gt; tail //head -&gt; 2 -&gt; 3 -&gt; 1 -&gt; tail //head -&gt; 4 -&gt; 2 -&gt; 3 -&gt; tail","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://juniousy.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【本科时期文章】生产者消费者模型的一个例子","slug":"存档/生产者消费者模型的一个变型","date":"2019-03-02T14:08:53.000Z","updated":"2022-05-30T09:06:50.985Z","comments":true,"path":"2019/03/02/存档/生产者消费者模型的一个变型/","link":"","permalink":"https://juniousy.github.io/2019/03/02/%E5%AD%98%E6%A1%A3/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8F%98%E5%9E%8B/","excerpt":"一般的生产者消费者模型中，生产者和消费者都是尽可能快地处理任务。但在工作中，我遇到了一种情况，需要每个消费者尽可能多地解决一批任务，这样可以打包处理，降低I/O频次。我当时用的方法是在消费者端给BlockingQueue加锁。后来想想这种方法多余了。这篇文章一是讨论一下这种方法，作个反思，二来作为新博客的第一篇文章，起个开头。","text":"一般的生产者消费者模型中，生产者和消费者都是尽可能快地处理任务。但在工作中，我遇到了一种情况，需要每个消费者尽可能多地解决一批任务，这样可以打包处理，降低I/O频次。我当时用的方法是在消费者端给BlockingQueue加锁。后来想想这种方法多余了。这篇文章一是讨论一下这种方法，作个反思，二来作为新博客的第一篇文章，起个开头。 模拟当时的解决方法用来解决生产者消费者问题的BlockingQueue： private static final BlockingQueue&lt;Task&gt; taskBlockingQueue = new ArrayBlockingQueue&lt;&gt;(100); 生产者部分没有什么区别，直接往队列里添加任务： private void produce(int taskId) &#123; try &#123; taskBlockingQueue.put(new Task(taskId)); System.out.println(String.format(&quot;生产者%d\\t添加任务%d&quot;, id, taskId)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 消费者部分在打包的过程中都对阻塞队列加锁，不允许其他消费者获取任务： private static final Lock packageLock = new ReentrantLock(); 消费者需要在指定时间内打包，超时则退出这轮消费。 private void consume() &#123; try &#123; if (packageLock.tryLock(5, TimeUnit.MILLISECONDS)) &#123; try &#123; doPackage(); &#125; finally &#123; packageLock.unlock(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void doPackage() &#123; long start = System.currentTimeMillis(); long end; int packageNum = 0; for (int i = 0; i &lt; consumerPackageSize; i++) &#123; doConsume(); packageNum++; end = System.currentTimeMillis(); if (end - start &gt; packageTime) &#123; break; &#125; &#125; end = System.currentTimeMillis(); System.out.println(String.format(&quot;消费者%d\\t打包%d个\\t耗时%d&quot;, id, packageNum, end - start)); &#125; private void doConsume() &#123; Task task = null; try &#123; task = taskBlockingQueue.poll(100, TimeUnit.MILLISECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (task == null) &#123; return; &#125; System.out.println(String.format(&quot;消费者%d\\t完成任务%d&quot;, id, task.getId())); &#125; 整个完整的测试类： import lombok.AllArgsConstructor; import lombok.Data; import java.util.Random; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; import java.util.concurrent.TimeUnit; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; /** * @author Junious * @date 2019/02/25 **/ public class ProducerAndConsumerTest &#123; private static final int producerNumber = 5; private static final int consumerNumber = 5; private static final BlockingQueue&lt;Task&gt; taskBlockingQueue = new ArrayBlockingQueue&lt;&gt;(100); private static final Lock packageLock = new ReentrantLock(); private static final int consumerPackageSize = 20; private static final int packageTime = 2000; public static void main(String[] args) &#123; Runtime.getRuntime().addShutdownHook( new Thread(() -&gt; System.out.println (&quot;queue size:&quot; + taskBlockingQueue.size())) ); ProducerAndConsumerTest test = new ProducerAndConsumerTest(); test.init(); &#125; private void init() &#123; //add producers for (int i = 0; i &lt; producerNumber; i++) &#123; Thread t = new Thread(new Producer(i)); t.start(); &#125; //add consumers for (int i = 0; i &lt; consumerNumber; i++) &#123; Thread t = new Thread(new Consumer(i)); t.start(); &#125; &#125; @AllArgsConstructor @Data class Producer implements Runnable &#123; private int id; @Override public void run() &#123; Random random = new Random(); while (true) &#123; int taskId = random.nextInt(1000) + 1; produce(taskId); try &#123; Thread.sleep(random.nextInt(300) + 400); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); break; &#125; &#125; &#125; private void produce(int taskId) &#123; try &#123; taskBlockingQueue.put(new Task(taskId)); System.out.println(String.format(&quot;生产者%d\\t添加任务%d&quot;, id, taskId)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @AllArgsConstructor @Data class Consumer implements Runnable &#123; private int id; @Override public void run() &#123; Random random = new Random(); while (true) &#123; consume(); try &#123; Thread.sleep(random.nextInt(300) + 400); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); break; &#125; &#125; &#125; private void consume() &#123; try &#123; if (packageLock.tryLock(5, TimeUnit.MILLISECONDS)) &#123; try &#123; doPackage(); &#125; finally &#123; packageLock.unlock(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void doPackage() &#123; long start = System.currentTimeMillis(); long end; int packageNum = 0; for (int i = 0; i &lt; consumerPackageSize; i++) &#123; doConsume(); packageNum++; end = System.currentTimeMillis(); if (end - start &gt; packageTime) &#123; break; &#125; &#125; end = System.currentTimeMillis(); System.out.println(String.format(&quot;消费者%d\\t打包%d个\\t耗时%d&quot;, id, packageNum, end - start)); &#125; private void doConsume() &#123; Task task = null; try &#123; task = taskBlockingQueue.poll(100, TimeUnit.MILLISECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (task == null) &#123; return; &#125; System.out.println(String.format(&quot;消费者%d\\t完成任务%d&quot;, id, task.getId())); &#125; &#125; &#125; @AllArgsConstructor @Data class Task &#123; private int id; &#125; 截取一段测试结果： 生产者0 添加任务545 生产者2 添加任务943 生产者1 添加任务359 生产者3 添加任务97 生产者4 添加任务705 消费者2 完成任务545 消费者2 完成任务359 消费者2 完成任务943 消费者2 完成任务97 消费者2 完成任务705 生产者2 添加任务32 消费者2 完成任务32 生产者4 添加任务488 消费者2 完成任务488 生产者1 添加任务691 消费者2 完成任务691 消费者2 完成任务3 生产者3 添加任务3 生产者0 添加任务815 消费者2 完成任务815 消费者2 完成任务290 生产者1 添加任务290 消费者2 完成任务408 生产者2 添加任务408 消费者2 完成任务873 生产者3 添加任务873 消费者2 打包20个 耗时1165 生产者0 添加任务852 消费者1 完成任务852 消费者1 完成任务743 生产者4 添加任务743 生产者0 添加任务114 消费者1 完成任务114 生产者1 添加任务454 消费者1 完成任务454 消费者1 完成任务920 生产者2 添加任务920 生产者3 添加任务847 消费者1 完成任务847 生产者4 添加任务905 消费者1 完成任务905 生产者3 添加任务698 消费者1 完成任务698 生产者1 添加任务372 消费者1 完成任务372 生产者0 添加任务568 消费者1 完成任务568 生产者2 添加任务419 消费者1 完成任务419 生产者4 添加任务417 消费者1 完成任务417 消费者1 打包20个 耗时1295 生产者3 添加任务888 消费者0 完成任务888 生产者1 添加任务189 消费者0 完成任务189 生产者2 添加任务892 消费者0 完成任务892 生产者4 添加任务375 消费者0 完成任务375 生产者0 添加任务723 消费者0 完成任务723 生产者3 添加任务543 消费者0 完成任务543 消费者0 完成任务205 生产者1 添加任务205 生产者2 添加任务657 消费者0 完成任务657 生产者0 添加任务549 消费者0 完成任务549 生产者4 添加任务812 消费者0 完成任务812 生产者3 添加任务737 消费者0 完成任务737 消费者0 打包20个 耗时1208 生产者2 添加任务784 消费者4 完成任务784 生产者1 添加任务252 消费者4 完成任务252 生产者0 添加任务622 消费者4 完成任务622 生产者4 添加任务524 消费者4 完成任务524 生产者3 添加任务73 消费者4 完成任务73 生产者2 添加任务491 消费者4 完成任务491 生产者0 添加任务225 消费者4 完成任务225 生产者1 添加任务207 消费者4 完成任务207 生产者4 添加任务326 消费者4 完成任务326 生产者2 添加任务983 消费者4 完成任务983 生产者0 添加任务865 消费者4 完成任务865 生产者3 添加任务347 消费者4 完成任务347 消费者4 打包20个 耗时1318 可以看到每次打包只有一个消费者在进行消费，其实相当于只有一个消费者线程，等于没有使用并发。当消费者任务很耗时时： private void doConsume() &#123; Task task = null; try &#123; task = taskBlockingQueue.poll(100, TimeUnit.MILLISECONDS); //模拟耗时 Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (task == null) &#123; return; &#125; System.out.println(String.format(&quot;消费者%d\\t完成任务%d&quot;, id, task.getId())); &#125; 这时候在中止时可以看到队列有10到30不等的Task暂留。模拟耗时越长，暂留的越多，也就是相当于性能越差。 解决方案实际上不需要加锁，设定一个超时时间即可。 @AllArgsConstructor @Data class Consumer2 implements Runnable &#123; private int id; @Override public void run() &#123; Random random = new Random(); while (true) &#123; try &#123; consume(); Thread.sleep(random.nextInt(300) + 400); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); break; &#125; &#125; &#125; private void consume() throws InterruptedException &#123; long start = System.currentTimeMillis(); ArrayList&lt;Task&gt; tasks = new ArrayList&lt;&gt;(); long end; int packageNum = 0; for (int i = 0; i &lt; consumerPackageSize; i++) &#123; //模拟打包任务 Task task = taskBlockingQueue.poll(packageTime, TimeUnit.MILLISECONDS); if (task == null) &#123; continue; &#125; Thread.sleep(200); tasks.add(task); packageNum++; end = System.currentTimeMillis(); if (end - start &gt; packageTime) &#123; break; &#125; &#125; end = System.currentTimeMillis(); System.out.println(String.format(&quot;消费者%d\\t打包%d个\\t耗时%d\\t%s&quot;, id, packageNum, end - start, tasks)); &#125; &#125; 生产者2 添加任务539 生产者0 添加任务319 生产者1 添加任务655 生产者4 添加任务843 生产者3 添加任务788 生产者0 添加任务716 生产者3 添加任务176 生产者4 添加任务735 生产者2 添加任务7 生产者1 添加任务283 生产者4 添加任务466 生产者3 添加任务486 生产者0 添加任务649 生产者2 添加任务373 生产者1 添加任务158 生产者0 添加任务532 生产者4 添加任务914 生产者1 添加任务734 生产者3 添加任务571 生产者2 添加任务114 生产者1 添加任务340 生产者3 添加任务670 生产者0 添加任务482 生产者2 添加任务298 生产者4 添加任务598 消费者0 打包5个 耗时2213 [Task(id=655), Task(id=716), Task(id=466), Task(id=532), Task(id=340)] 消费者4 打包5个 耗时2280 [Task(id=319), Task(id=176), Task(id=486), Task(id=914), Task(id=670)] 消费者1 打包5个 耗时2328 [Task(id=788), Task(id=735), Task(id=649), Task(id=734), Task(id=482)] 消费者3 打包5个 耗时2351 [Task(id=843), Task(id=7), Task(id=373), Task(id=571), Task(id=298)] 消费者2 打包5个 耗时2400 [Task(id=539), Task(id=283), Task(id=158), Task(id=114), Task(id=598)] 生产者3 添加任务928 生产者0 添加任务360 生产者1 添加任务724 生产者2 添加任务539 生产者4 添加任务926 生产者3 添加任务206 生产者0 添加任务596 生产者1 添加任务841 生产者4 添加任务834 生产者2 添加任务340 生产者3 添加任务585 生产者1 添加任务500 生产者4 添加任务532 生产者0 添加任务800 生产者2 添加任务914 生产者3 添加任务202 生产者1 添加任务850 生产者0 添加任务506 生产者1 添加任务785 生产者2 添加任务633 生产者4 添加任务182 生产者3 添加任务154 生产者0 添加任务13 生产者2 添加任务880 消费者3 打包5个 耗时2199 [Task(id=724), Task(id=841), Task(id=532), Task(id=506), Task(id=13)] 生产者4 添加任务214 消费者0 打包5个 耗时2217 [Task(id=539), Task(id=834), Task(id=800), Task(id=785), Task(id=880)] 生产者1 添加任务789 生产者3 添加任务786 消费者4 打包6个 耗时2583 [Task(id=928), Task(id=926), Task(id=340), Task(id=914), Task(id=633), Task(id=214)] 生产者0 添加任务468 生产者2 添加任务189 消费者1 打包6个 耗时2597 [Task(id=360), Task(id=206), Task(id=585), Task(id=202), Task(id=182), Task(id=789)] 消费者2 打包5个 耗时2465 [Task(id=596), Task(id=500), Task(id=850), Task(id=154), Task(id=786)] 生产者4 添加任务812 生产者0 添加任务239 生产者3 添加任务671 生产者1 添加任务730 生产者2 添加任务124 生产者4 添加任务679 生产者0 添加任务320 生产者2 添加任务917 生产者1 添加任务986 生产者3 添加任务557 生产者0 添加任务415 生产者4 添加任务559 生产者2 添加任务880 生产者1 添加任务920 生产者3 添加任务502 生产者0 添加任务679 生产者4 添加任务823 生产者2 添加任务594 生产者1 添加任务336 生产者3 添加任务502 生产者0 添加任务453 生产者4 添加任务360 消费者0 打包6个 耗时2249 [Task(id=468), Task(id=239), Task(id=679), Task(id=415), Task(id=679), Task(id=453)] 消费者3 打包6个 耗时2320 [Task(id=189), Task(id=671), Task(id=320), Task(id=559), Task(id=823), Task(id=360)] 生产者2 添加任务823 生产者3 添加任务857 生产者1 添加任务395 生产者0 添加任务937 生产者4 添加任务817 消费者2 打包4个 耗时2264 [Task(id=917), Task(id=880), Task(id=594), Task(id=823)] 消费者4 打包6个 耗时2622 [Task(id=812), Task(id=730), Task(id=986), Task(id=920), Task(id=336), Task(id=857)] 消费者1 打包5个 耗时2408 [Task(id=124), Task(id=557), Task(id=502), Task(id=502), Task(id=395)] 这时候中止可以看到队列几乎没有Task暂留 当设置消费者消费时间为1000ms时，运行一段时间队列就满了，这时候是当增加消费者线程数即可让任务处理跟上生产者的生产速度。","categories":[{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://juniousy.github.io/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://juniousy.github.io/tags/%E5%B9%B6%E5%8F%91/"}]}],"categories":[{"name":"Lab","slug":"Lab","permalink":"https://juniousy.github.io/categories/Lab/"},{"name":"开发","slug":"开发","permalink":"https://juniousy.github.io/categories/%E5%BC%80%E5%8F%91/"},{"name":"刷题","slug":"刷题","permalink":"https://juniousy.github.io/categories/%E5%88%B7%E9%A2%98/"}],"tags":[{"name":"distributed system","slug":"distributed-system","permalink":"https://juniousy.github.io/tags/distributed-system/"},{"name":"算法","slug":"算法","permalink":"https://juniousy.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"https://juniousy.github.io/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://juniousy.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"Redis","slug":"Redis","permalink":"https://juniousy.github.io/tags/Redis/"}]}